
    
            <!DOCTYPE html>
        <html lang="en">

        <head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TFCQEJR7TD"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TFCQEJR7TD');
</script>
            <title>How Do I Deploy a Machine Learning Model on Kubernetes with TensorFlow Serving?</title>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Common CSS & Icons -->
<link rel="icon" href="/favicon.ico" type="image/x-icon">
<link rel="stylesheet" href="/assets/plugins/highlight/styles/monokai-sublime.css">
<link id="theme-style" rel="stylesheet" href="/assets/css/theme-8.css">
<link rel="stylesheet" href="/assets/css/post.css">
            <meta name="description" content="Learn to deploy machine learning models on Kubernetes using TensorFlow Serving. Step-by-step guide for seamless deployment.">

            <div id="head-placeholder"></div>
            <script src="/assets/js/blog.js" defer></script>
            <link rel="stylesheet" href="/assets/css/post.css">
        </head>

        <body>

            <div id="header-placeholder"></div>

            <div class="main-wrapper">

                <article class="blog-post px-3 py-5 p-md-5">
                    <div class="container single-col-max-width">
                        <header class="blog-post-header">
                            <h1 class="title mb-2">How Do I Deploy a Machine Learning Model on Kubernetes with TensorFlow Serving?</h1>
                        </header>

                        <div class="blog-post-body">
                            <p>Deploying a machine learning model on Kubernetes with TensorFlow
Serving is about packaging the model. This way, we can serve it as an
API. It lets applications make predictions or inference requests.
TensorFlow Serving is a flexible and fast system for serving machine
learning models. It is made for production environments. Kubernetes
helps us manage containerized applications easily.</p>
<p>In this article, we will talk about the key steps to deploy a machine
learning model using TensorFlow Serving on a Kubernetes cluster. We will
look at what we need before we start. We will also see how to prepare
the model, create a Docker image, set up a Kubernetes cluster, and
expose the TensorFlow Serving API. Plus, we will discuss real-life use
cases, ways to monitor and scale, and answer common questions about this
deployment process.</p>
<ul>
<li>How Can I Deploy a Machine Learning Model on Kubernetes Using
TensorFlow Serving?</li>
<li>What Are the Prerequisites for Deploying TensorFlow Serving on
Kubernetes?</li>
<li>How Do I Prepare My Machine Learning Model for TensorFlow
Serving?</li>
<li>How Do I Create a Docker Image for TensorFlow Serving?</li>
<li>How Can I Set Up a Kubernetes Cluster for TensorFlow Serving?</li>
<li>What Are the Steps to Deploy TensorFlow Serving on Kubernetes?</li>
<li>How Do I Expose My TensorFlow Serving API on Kubernetes?</li>
<li>What Are Real Life Use Cases for TensorFlow Serving on
Kubernetes?</li>
<li>How Can I Monitor and Scale My TensorFlow Serving Deployment?</li>
<li>Frequently Asked Questions</li>
</ul>
<p>If you want to understand more about Kubernetes and its features, you
can check this article on <a
href="https://bestonlinetutorial.com/kubernetes/what-is-kubernetes-and-how-does-it-simplify-container-management.html">What
is Kubernetes and How Does it Simplify Container Management?</a>.</p>
<h2
id="what-are-the-prerequisites-for-deploying-tensorflow-serving-on-kubernetes">What
Are the Prerequisites for Deploying TensorFlow Serving on
Kubernetes?</h2>
<p>Before we deploy TensorFlow Serving on Kubernetes, we need to check
some important things.</p>
<ol type="1">
<li><p><strong>Kubernetes Cluster</strong>: We must have a running
Kubernetes cluster. We can create a local cluster with Minikube or use a
cloud service like AWS EKS, Google GKE, or Azure AKS. If we need help
with setting up a Kubernetes cluster, we can look at this <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-set-up-a-kubernetes-cluster-on-aws-eks.html">link</a>.</p></li>
<li><p><strong>kubectl</strong>: We need to install the Kubernetes
command-line tool called <code>kubectl</code>. This tool helps us to
talk to our Kubernetes cluster. We can find installation instructions <a
href="https://bestonlinetutorial.com/kubernetes/what-is-kubectl-and-how-do-i-use-it-to-manage-kubernetes.html">here</a>.</p></li>
<li><p><strong>Docker</strong>: We must also install Docker on our
computer. This is needed to build the TensorFlow Serving Docker image.
We can check the installation guide <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-install-minikube-for-local-kubernetes-development.html">here</a>.</p></li>
<li><p><strong>TensorFlow Model</strong>: We need a trained TensorFlow
model that is saved in the <code>SavedModel</code> format. This format
lets TensorFlow Serving load the model correctly.</p></li>
<li><p><strong>Resource Configuration</strong>: We have to make sure our
cluster has enough resources like CPU and memory to run TensorFlow
Serving. We can learn how to manage resource limits and requests in this
<a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-manage-resource-limits-and-requests-in-kubernetes.html">article</a>.</p></li>
<li><p><strong>Networking</strong>: It is good to have a basic
understanding of Kubernetes networking. We should know about services
and ingress. This helps us expose our TensorFlow Serving API. For more
information, we can check this resource about <a
href="https://bestonlinetutorial.com/kubernetes/how-does-kubernetes-networking-work.html">Kubernetes
networking</a>.</p></li>
<li><p><strong>Permissions</strong>: We should check if we have the
right permissions to deploy resources in our Kubernetes cluster. This is
especially important when we use a managed service.</p></li>
</ol>
<p>If we meet these prerequisites, we will be ready to deploy TensorFlow
Serving on Kubernetes in a good way.</p>
<h2
id="how-do-we-prepare-our-machine-learning-model-for-tensorflow-serving">How
Do We Prepare Our Machine Learning Model for TensorFlow Serving?</h2>
<p>To prepare a machine learning model for TensorFlow Serving, we must
convert it to the SavedModel format. This is the standard format that
TensorFlow Serving uses. Here are the main steps we should follow:</p>
<ol type="1">
<li><p><strong>Train Our Model</strong>: We can use TensorFlow to train
our machine learning model. For example, if we are using a simple neural
network, it may look like this:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a simple model</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Sequential([</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>(<span class="dv">784</span>,)),</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    keras.layers.Dense(<span class="dv">10</span>, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>              loss<span class="op">=</span><span class="st">&#39;sparse_categorical_crossentropy&#39;</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>              metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>model.fit(train_images, train_labels, epochs<span class="op">=</span><span class="dv">5</span>)</span></code></pre></div></li>
<li><p><strong>Export the Model</strong>: After we train the model, we
can save it in the SavedModel format. We do this using the
<code>tf.saved_model.save</code> function:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>tf.saved_model.save(model, <span class="st">&#39;/path/to/saved_model/my_model&#39;</span>)</span></code></pre></div></li>
<li><p><strong>Versioning</strong>: It is a good idea to version our
models. We create a folder to hold the model with a version number:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> /path/to/saved_model/1</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> <span class="at">-r</span> /path/to/saved_model/my_model/<span class="pp">*</span> /path/to/saved_model/1/</span></code></pre></div></li>
<li><p><strong>Model Signature</strong>: We need to define the model
input and output signatures. This helps TensorFlow Serving know how to
handle requests. For example:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="at">@tf.function</span>(input_signature<span class="op">=</span>[tf.TensorSpec(shape<span class="op">=</span>[<span class="va">None</span>, <span class="dv">784</span>], dtype<span class="op">=</span>tf.float32)])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> predict(input_tensor):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model(input_tensor)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>tf.saved_model.save(model, <span class="st">&#39;/path/to/saved_model/my_model&#39;</span>, signatures<span class="op">=</span>{<span class="st">&#39;serving_default&#39;</span>: predict})</span></code></pre></div></li>
<li><p><strong>Verify the SavedModel</strong>: We can load our model to
check if it works correctly:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>loaded_model <span class="op">=</span> tf.saved_model.load(<span class="st">&#39;/path/to/saved_model/my_model&#39;</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>infer <span class="op">=</span> loaded_model.signatures[<span class="st">&#39;serving_default&#39;</span>]</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> infer(tf.constant(test_data))  <span class="co"># test_data should have the same shape as input</span></span></code></pre></div></li>
</ol>
<p>By following these steps, we can prepare our machine learning model
for TensorFlow Serving. This will make sure our model can handle
inference requests well. If we want to learn more about deploying
machine learning models on Kubernetes, we should check <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-deploy-machine-learning-models-on-kubernetes.html">this
guide</a>.</p>
<h2 id="how-do-i-create-a-docker-image-for-tensorflow-serving">How Do I
Create a Docker Image for TensorFlow Serving?</h2>
<p>To create a Docker image for TensorFlow Serving, we can follow these
steps:</p>
<ol type="1">
<li><p><strong>Set Up Your Environment</strong>: First, we need to have
Docker on our computer. We can check if it is installed by running this
command:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> <span class="at">--version</span></span></code></pre></div></li>
<li><p><strong>Create a Dockerfile</strong>: In our project folder, we
should make a file called <code>Dockerfile</code>. We can put this
content in it:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode dockerfile"><code class="sourceCode dockerfile"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Use the official TensorFlow Serving base image</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">FROM</span> tensorflow/serving:latest</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Copy our model files to the Docker image</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="kw">COPY</span> ./my_model /models/my_model</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Specify the model name (this must match the folder name)</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="kw">ENV</span> MODEL_NAME=my_model</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Start TensorFlow Serving</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="kw">CMD</span> [<span class="st">&quot;tensorflow_model_server&quot;</span>, <span class="st">&quot;--model_name=${MODEL_NAME}&quot;</span>, <span class="st">&quot;--model_base_path=/models/${MODEL_NAME}&quot;</span>]</span></code></pre></div>
<p>We should replace <code>./my_model</code> with the path to our
TensorFlow model folder.</p></li>
<li><p><strong>Build the Docker Image</strong>: We can run this command
in the terminal from the folder that has our
<code>Dockerfile</code>:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> build <span class="at">-t</span> my-tf-serving-image .</span></code></pre></div>
<p>This command makes the Docker image and names it
<code>my-tf-serving-image</code>.</p></li>
<li><p><strong>Verify the Image Creation</strong>: After the build is
done, we can list our Docker images to check:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> images</span></code></pre></div></li>
<li><p><strong>Run the Docker Container</strong>: To run the TensorFlow
Serving container, we use this command. It opens port 8501, which is the
default port for the TensorFlow Serving REST API:</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">docker</span> run <span class="at">-p</span> 8501:8501 <span class="at">--name</span><span class="op">=</span>tf_serving_container my-tf-serving-image</span></code></pre></div></li>
<li><p><strong>Testing the API</strong>: Once the container is running,
we can test the TensorFlow Serving API with a <code>curl</code> command
or any HTTP client:</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-d</span> <span class="st">&#39;{&quot;signature_name&quot;:&quot;serving_default&quot;, &quot;instances&quot;:[{&quot;input_data&quot;: [your_input_data]}]}&#39;</span> <span class="at">-H</span> <span class="st">&quot;Content-Type: application/json&quot;</span> <span class="at">-X</span> POST http://localhost:8501/v1/models/my_model:predict</span></code></pre></div>
<p>We need to replace <code>your_input_data</code> with the actual input
data that our model needs.</p></li>
</ol>
<p>By following these steps, we can create and run a Docker image for
TensorFlow Serving. This lets us serve our machine learning models well
on Kubernetes. For more info on deploying machine learning models on
Kubernetes, we can check this article on <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-deploy-machine-learning-models-on-kubernetes.html">how
to deploy machine learning models on Kubernetes</a>.</p>
<h2
id="how-can-we-set-up-a-kubernetes-cluster-for-tensorflow-serving">How
Can We Set Up a Kubernetes Cluster for TensorFlow Serving?</h2>
<p>To set up a Kubernetes cluster for TensorFlow Serving, we can follow
these steps. We will look at both local and cloud-based setups.</p>
<h3 id="local-setup-with-minikube">1. Local Setup with Minikube</h3>
<ol type="1">
<li><p><strong>Install Minikube</strong>: First, we need to download and
install Minikube from the <a
href="https://minikube.sigs.k8s.io/docs/start/">official
site</a>.</p></li>
<li><p><strong>Start Minikube</strong>:</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">minikube</span> start</span></code></pre></div></li>
<li><p><strong>Configure kubectl</strong>: We need to make sure that we
have <code>kubectl</code> installed. This helps us to work with our
Minikube cluster.</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get nodes</span></code></pre></div></li>
</ol>
<h3 id="cloud-based-setup-aws-eks-example">2. Cloud-Based Setup (AWS EKS
Example)</h3>
<ol type="1">
<li><p><strong>Install AWS CLI</strong>: We should install the AWS CLI
and set it up with our credentials.</p></li>
<li><p><strong>Create EKS Cluster</strong>:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">eksctl</span> create cluster <span class="at">--name</span> tensorflow-serving-cluster <span class="at">--region</span> us-west-2 <span class="at">--nodegroup-name</span> standard-workers <span class="at">--node-type</span> t2.medium <span class="at">--nodes</span> 2</span></code></pre></div></li>
<li><p><strong>Update kubeconfig</strong>:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">aws</span> eks <span class="at">--region</span> us-west-2 update-kubeconfig <span class="at">--name</span> tensorflow-serving-cluster</span></code></pre></div></li>
<li><p><strong>Verify Cluster</strong>:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get svc</span></code></pre></div></li>
</ol>
<h3 id="using-google-kubernetes-engine-gke">3. Using Google Kubernetes
Engine (GKE)</h3>
<ol type="1">
<li><p><strong>Install Google Cloud SDK</strong>: We need to make sure
we have the Google Cloud SDK installed.</p></li>
<li><p><strong>Create GKE Cluster</strong>:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="ex">gcloud</span> container clusters create tensorflow-serving-cluster <span class="at">--zone</span> us-central1-a <span class="at">--num-nodes</span> 2</span></code></pre></div></li>
<li><p><strong>Get Credentials</strong>:</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ex">gcloud</span> container clusters get-credentials tensorflow-serving-cluster <span class="at">--zone</span> us-central1-a</span></code></pre></div></li>
<li><p><strong>Check Cluster</strong>:</p>
<div class="sourceCode" id="cb19"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get nodes</span></code></pre></div></li>
</ol>
<h3 id="using-azure-kubernetes-service-aks">4. Using Azure Kubernetes
Service (AKS)</h3>
<ol type="1">
<li><p><strong>Install Azure CLI</strong>: It is important that we have
the Azure CLI installed.</p></li>
<li><p><strong>Create AKS Cluster</strong>:</p>
<div class="sourceCode" id="cb20"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> aks create <span class="at">--resource-group</span> myResourceGroup <span class="at">--name</span> tensorflow-serving-cluster <span class="at">--node-count</span> 2 <span class="at">--enable-addons</span> monitoring <span class="at">--generate-ssh-keys</span></span></code></pre></div></li>
<li><p><strong>Connect to the Cluster</strong>:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">az</span> aks get-credentials <span class="at">--resource-group</span> myResourceGroup <span class="at">--name</span> tensorflow-serving-cluster</span></code></pre></div></li>
<li><p><strong>Verify Connection</strong>:</p>
<div class="sourceCode" id="cb22"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get nodes</span></code></pre></div></li>
</ol>
<h3 id="final-checks">Final Checks</h3>
<p>No matter which method we use, we should check if we can access the
Kubernetes API. We also want to make sure our cluster is running well.
To do this, we can use the command:</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> cluster-info</span></code></pre></div>
<p>This setup gives us a good start for deploying TensorFlow Serving in
our Kubernetes environment. If we want to learn more about managing
Kubernetes clusters, we can check <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-set-up-a-kubernetes-cluster-on-aws-eks.html">how
do I set up a Kubernetes cluster on AWS EKS</a>.</p>
<h2
id="what-are-the-steps-to-deploy-tensorflow-serving-on-kubernetes">What
Are the Steps to Deploy TensorFlow Serving on Kubernetes?</h2>
<p>To deploy TensorFlow Serving on Kubernetes, we can follow these
steps:</p>
<ol type="1">
<li><p><strong>Access Your Kubernetes Cluster</strong>: Make sure your
Kubernetes cluster is running. We need to access it using
<code>kubectl</code>.</p></li>
<li><p><strong>Create a Model Directory</strong>: We should organize our
model files in a directory. For example:</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> /models/my_model</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cp</span> my_model/saved_model.pb /models/my_model/</span></code></pre></div></li>
<li><p><strong>Create a Deployment YAML File</strong>: We define the
deployment for TensorFlow Serving in a YAML file called
<code>tf-serving-deployment.yaml</code>:</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Deployment</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tf-serving</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">replicas</span><span class="kw">:</span><span class="at"> </span><span class="dv">1</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">matchLabels</span><span class="kw">:</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">app</span><span class="kw">:</span><span class="at"> tf-serving</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">template</span><span class="kw">:</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">labels</span><span class="kw">:</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">app</span><span class="kw">:</span><span class="at"> tf-serving</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tf-serving</span></span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">image</span><span class="kw">:</span><span class="at"> tensorflow/serving:latest</span></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">containerPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8501</span></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">volumeMounts</span><span class="kw">:</span></span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> model-volume</span></span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">mountPath</span><span class="kw">:</span><span class="at"> /models/my_model</span></span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">env</span><span class="kw">:</span></span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> MODEL_NAME</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">value</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;my_model&quot;</span></span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">volumes</span><span class="kw">:</span></span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> model-volume</span></span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="fu">hostPath</span><span class="kw">:</span></span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">path</span><span class="kw">:</span><span class="at"> /models</span></span></code></pre></div></li>
<li><p><strong>Deploy TensorFlow Serving</strong>: We run the following
command to create the deployment:</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> apply <span class="at">-f</span> tf-serving-deployment.yaml</span></code></pre></div></li>
<li><p><strong>Create a Service YAML File</strong>: We expose the
TensorFlow Serving deployment using a service. We create
<code>tf-serving-service.yaml</code>:</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tf-serving</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8501</span></span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">targetPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8501</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> tf-serving</span></span></code></pre></div></li>
<li><p><strong>Deploy the Service</strong>: We execute the following
command to create the service:</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> apply <span class="at">-f</span> tf-serving-service.yaml</span></code></pre></div></li>
<li><p><strong>Check Deployment and Service Status</strong>: We verify
that both the deployment and the service are running:</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get deployments</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get services</span></code></pre></div></li>
<li><p><strong>Access the TensorFlow Serving API</strong>: If we use a
LoadBalancer, we can get the external IP with:</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get service tf-serving</span></code></pre></div>
<p>Now we can access the TensorFlow Serving API at
<code>http://&lt;EXTERNAL_IP&gt;:8501/v1/models/my_model</code>.</p></li>
<li><p><strong>Test the API</strong>: We use <code>curl</code> to test
the endpoint:</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-d</span> <span class="st">&#39;{&quot;signature_name&quot;:&quot;serving_default&quot;, &quot;instances&quot;:[{&quot;input_tensor&quot;:[value]}]}&#39;</span> <span class="at">-H</span> <span class="st">&quot;Content-Type: application/json&quot;</span> <span class="at">-X</span> POST http://<span class="op">&lt;</span>EXTERNAL_IP<span class="op">&gt;</span>:8501/v1/models/my_model:predict</span></code></pre></div></li>
</ol>
<p>This way we can deploy TensorFlow Serving on Kubernetes. Now we can
access our machine learning model through a strong and scalable API. For
more help on setting up your Kubernetes cluster, look at <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-set-up-a-kubernetes-cluster-on-aws-eks.html">how
to set up a Kubernetes cluster on AWS EKS</a>.</p>
<h2 id="how-do-we-expose-our-tensorflow-serving-api-on-kubernetes">How
Do We Expose Our TensorFlow Serving API on Kubernetes?</h2>
<p>To expose our TensorFlow Serving API on Kubernetes, we usually create
a Kubernetes Service. This service helps others access our TensorFlow
Serving deployment. Here are the steps to create a service that shows
our model.</p>
<ol type="1">
<li><strong>Create a Service YAML file</strong>: This file will explain
how our service will be shown. We save this as
<code>tensorflow-serving-service.yaml</code>.</li>
</ol>
<div class="sourceCode" id="cb32"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> v1</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Service</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tensorflow-serving</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> LoadBalancer</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">ports</span><span class="kw">:</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="kw">-</span><span class="at"> </span><span class="fu">port</span><span class="kw">:</span><span class="at"> </span><span class="dv">8501</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">targetPort</span><span class="kw">:</span><span class="at"> </span><span class="dv">8501</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">protocol</span><span class="kw">:</span><span class="at"> TCP</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">selector</span><span class="kw">:</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">app</span><span class="kw">:</span><span class="at"> tensorflow-serving</span></span></code></pre></div>
<ol start="2" type="1">
<li><strong>Deploy the Service</strong>: We use <code>kubectl</code> to
create the service in our Kubernetes cluster.</li>
</ol>
<div class="sourceCode" id="cb33"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> apply <span class="at">-f</span> tensorflow-serving-service.yaml</span></code></pre></div>
<ol start="3" type="1">
<li><strong>Verify the Service</strong>: We check if the service is
created and running.</li>
</ol>
<div class="sourceCode" id="cb34"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get services</span></code></pre></div>
<ol start="4" type="1">
<li><strong>Access the API</strong>: After the service is up and
running, we can access the TensorFlow Serving API using the external IP
from the LoadBalancer. We use this CURL command to send a request:</li>
</ol>
<div class="sourceCode" id="cb35"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-d</span> <span class="st">&#39;{&quot;signature_name&quot;:&quot;serving_default&quot;, &quot;instances&quot;:[{&quot;input&quot;: [1.0, 2.0, 5.0]}]}&#39;</span> <span class="dt">\</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">-H</span> <span class="st">&quot;Content-Type: application/json&quot;</span> <span class="dt">\</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">-X</span> POST http://<span class="op">&lt;</span>EXTERNAL_IP<span class="op">&gt;</span>:8501/v1/models/<span class="op">&lt;</span>MODEL_NAME<span class="op">&gt;</span>:predict</span></code></pre></div>
<p>We replace <code>&lt;EXTERNAL_IP&gt;</code> with the external IP
address of our service and <code>&lt;MODEL_NAME&gt;</code> with the name
of our deployed model.</p>
<ol start="5" type="1">
<li><strong>Using a NodePort Service (Optional)</strong>: If we do not
have a LoadBalancer service, we can use a NodePort service instead. We
change the service type in the YAML file:</li>
</ol>
<div class="sourceCode" id="cb36"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">type</span><span class="kw">:</span><span class="at"> NodePort</span></span></code></pre></div>
<p>After we deploy, we find the NodePort assigned to our service:</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> get services</span></code></pre></div>
<p>We can access the API using the node’s IP and the NodePort:</p>
<div class="sourceCode" id="cb38"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="ex">curl</span> <span class="at">-d</span> <span class="st">&#39;{&quot;signature_name&quot;:&quot;serving_default&quot;, &quot;instances&quot;:[{&quot;input&quot;: [1.0, 2.0, 5.0]}]}&#39;</span> <span class="dt">\</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">-H</span> <span class="st">&quot;Content-Type: application/json&quot;</span> <span class="dt">\</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">-X</span> POST http://<span class="op">&lt;</span>NODE_IP<span class="op">&gt;</span>:<span class="op">&lt;</span>NODE_PORT<span class="op">&gt;</span>/v1/models/<span class="op">&lt;</span>MODEL_NAME<span class="op">&gt;</span>:predict</span></code></pre></div>
<p>We replace <code>&lt;NODE_IP&gt;</code> and
<code>&lt;NODE_PORT&gt;</code> with the correct values.</p>
<p>This setup helps us expose our TensorFlow Serving API on Kubernetes.
Now, external applications can make predictions with our machine
learning model. For more details about Kubernetes services, we can check
<a
href="https://bestonlinetutorial.com/kubernetes/what-are-kubernetes-services-and-how-do-they-expose-applications.html">this
article</a>.</p>
<h2
id="what-are-real-life-use-cases-for-tensorflow-serving-on-kubernetes">What
Are Real Life Use Cases for TensorFlow Serving on Kubernetes?</h2>
<p>TensorFlow Serving on Kubernetes helps us to deploy machine learning
models in real life. Here are some important use cases:</p>
<ol type="1">
<li><strong>Image Recognition Services</strong>:
<ul>
<li>Companies like Google and Facebook use TensorFlow Serving to deploy
models that recognize and tag images quickly. This is very important for
platforms where users share content.<br />
</li>
<li><strong>Example</strong>: A photo-sharing app can use a model to
automatically tag images based on what is in them.</li>
</ul></li>
<li><strong>Natural Language Processing (NLP)</strong>:
<ul>
<li>Many organizations use NLP models with TensorFlow Serving to create
chatbots and virtual assistants. These models help us understand user
questions in real time.<br />
</li>
<li><strong>Example</strong>: A customer support chatbot that uses a
TensorFlow NLP model to answer user questions right away.</li>
</ul></li>
<li><strong>Recommendation Systems</strong>:
<ul>
<li>E-commerce sites use TensorFlow Serving to give personalized product
suggestions based on what users like and buy.<br />
</li>
<li><strong>Example</strong>: An online store can use a TensorFlow model
to look at user purchase history and recommend similar items.</li>
</ul></li>
<li><strong>Fraud Detection</strong>:
<ul>
<li>Banks and finance companies use TensorFlow Serving to find fake
transactions. They analyze patterns in transaction data.<br />
</li>
<li><strong>Example</strong>: A banking app that uses a TensorFlow model
to mark suspicious transactions for checking before they go
through.</li>
</ul></li>
<li><strong>Healthcare Diagnostics</strong>:
<ul>
<li>Hospitals and clinics use TensorFlow Serving to help diagnose health
problems through image analysis like X-rays or MRIs.<br />
</li>
<li><strong>Example</strong>: A tool that checks medical images to help
doctors find possible health issues.</li>
</ul></li>
<li><strong>Autonomous Vehicles</strong>:
<ul>
<li>Car companies use TensorFlow Serving for quick decisions in
self-driving cars. They process data from sensors to drive and avoid
obstacles.<br />
</li>
<li><strong>Example</strong>: A self-driving car that uses TensorFlow
models to understand data from cameras and other sensors for safe
driving.</li>
</ul></li>
<li><strong>Predictive Maintenance</strong>:
<ul>
<li>Factories use TensorFlow Serving to guess when machines will fail.
Models look at sensor data to tell when to do maintenance, so machines
don’t stop working.<br />
</li>
<li><strong>Example</strong>: A factory that uses TensorFlow models to
watch machines and predict when parts might break.</li>
</ul></li>
<li><strong>Video Analytics</strong>:
<ul>
<li>Security companies use TensorFlow Serving to check video feeds for
threats and monitor activities.<br />
</li>
<li><strong>Example</strong>: A security system that uses TensorFlow
models to find strange activities or unauthorized people.</li>
</ul></li>
</ol>
<p>These use cases show us how flexible TensorFlow Serving is on
Kubernetes. It helps organizations grow their machine learning
applications easily. For more details on how to deploy machine learning
models on Kubernetes, you can check out <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-deploy-machine-learning-models-on-kubernetes.html">this
article</a>.</p>
<h2
id="how-can-we-monitor-and-scale-our-tensorflow-serving-deployment">How
Can We Monitor and Scale Our TensorFlow Serving Deployment?</h2>
<p>Monitoring and scaling our TensorFlow Serving deployment on
Kubernetes is very important. It helps us keep good performance and
availability. Here are some easy steps and tools to watch and adjust our
deployment.</p>
<h3 id="monitoring">Monitoring</h3>
<ol type="1">
<li><p><strong>Prometheus and Grafana</strong>: We can use Prometheus to
collect metrics. Then, we use Grafana to see those metrics.</p>
<ul>
<li><p>To set up Prometheus, we can run this Helm command:</p>
<div class="sourceCode" id="cb39"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="ex">helm</span> install prometheus prometheus-community/prometheus</span></code></pre></div></li>
<li><p>For Grafana, we can visualize metrics with this command:</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="ex">helm</span> install grafana grafana/grafana</span></code></pre></div></li>
</ul></li>
<li><p><strong>Configure Metrics for TensorFlow Serving</strong>:
TensorFlow Serving gives us metrics in the Prometheus format. We need to
make sure our TensorFlow Serving container lets Prometheus get the
metrics:</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Deployment</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tensorflow-serving</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">template</span><span class="kw">:</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tensorflow-serving</span></span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> tensorflow/serving</span></span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">args</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="st">&quot;--model_name=my_model&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;--model_base_path=/models/my_model&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;--port=8501&quot;</span><span class="kw">,</span><span class="at"> </span><span class="st">&quot;--monitoring_port=8502&quot;</span><span class="kw">]</span></span></code></pre></div></li>
<li><p><strong>Set Up Alerts</strong>: We should create alert rules in
Prometheus. This way, we get notified if there are performance problems
like high latencies or error rates.</p></li>
</ol>
<h3 id="scaling">Scaling</h3>
<ol type="1">
<li><p><strong>Horizontal Pod Autoscaler (HPA)</strong>: We can
automatically scale our TensorFlow Serving pods based on CPU or memory
usage.</p>
<ul>
<li><p>To create an HPA resource, we run:</p>
<div class="sourceCode" id="cb42"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="ex">kubectl</span> autoscale deployment tensorflow-serving <span class="at">--cpu-percent</span><span class="op">=</span>50 <span class="at">--min</span><span class="op">=</span>1 <span class="at">--max</span><span class="op">=</span>10</span></code></pre></div></li>
</ul></li>
<li><p><strong>Resource Requests and Limits</strong>: We need to set
resource requests and limits in our TensorFlow Serving deployment. This
helps the HPA know what to do:</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="fu">apiVersion</span><span class="kw">:</span><span class="at"> apps/v1</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kind</span><span class="kw">:</span><span class="at"> Deployment</span></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="fu">metadata</span><span class="kw">:</span></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tensorflow-serving</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">template</span><span class="kw">:</span></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a><span class="at">    </span><span class="fu">spec</span><span class="kw">:</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a><span class="at">      </span><span class="fu">containers</span><span class="kw">:</span></span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a><span class="at">        </span><span class="kw">-</span><span class="at"> </span><span class="fu">name</span><span class="kw">:</span><span class="at"> tensorflow-serving</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">image</span><span class="kw">:</span><span class="at"> tensorflow/serving</span></span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a><span class="at">          </span><span class="fu">resources</span><span class="kw">:</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">requests</span><span class="kw">:</span></span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;500m&quot;</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;512Mi&quot;</span></span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a><span class="at">            </span><span class="fu">limits</span><span class="kw">:</span></span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">cpu</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;1000m&quot;</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a><span class="at">              </span><span class="fu">memory</span><span class="kw">:</span><span class="at"> </span><span class="st">&quot;1Gi&quot;</span></span></code></pre></div></li>
<li><p><strong>Cluster Autoscaler</strong>: We need to make sure our
Kubernetes cluster can grow or shrink based on the load.</p>
<ul>
<li>We can deploy the Cluster Autoscaler for our cloud provider like AWS
or GCP.</li>
</ul></li>
<li><p><strong>Load Testing</strong>: We can use tools like Apache
JMeter or Locust. These help us simulate traffic and check how our
TensorFlow Serving deployment performs under load.</p></li>
</ol>
<p>By using monitoring and scaling strategies, we can keep our
TensorFlow Serving deployment strong and responsive. It can handle
different loads well. For more details on how to connect monitoring
tools, we can refer to how to <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-monitor-a-kubernetes-application-with-prometheus-and-grafana.html">monitor
a Kubernetes application with Prometheus and Grafana</a>.</p>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<h3
id="what-is-tensorflow-serving-and-how-does-it-work-with-kubernetes">1.
What is TensorFlow Serving and how does it work with Kubernetes?</h3>
<p>We can say that TensorFlow Serving is a free library. It helps to
serve machine learning models in production. It lets users deploy models
easily. It also provides a strong API for communication. When we use it
with Kubernetes, TensorFlow Serving uses Kubernetes’ ability to manage
things. This helps with automatic scaling, load balancing, and easier
management of machine learning models in containers.</p>
<h3
id="how-do-i-optimize-a-tensorflow-serving-model-for-kubernetes-deployment">2.
How do I optimize a TensorFlow Serving model for Kubernetes
deployment?</h3>
<p>To make a TensorFlow Serving model better for deployment on
Kubernetes, we should convert the model to the TensorFlow SavedModel
format. We also need to package it properly in a Docker container. We
can use Kubernetes features like horizontal pod autoscaling. Also, we
should set resource requests and limits to manage resources well. This
helps our application run smoothly and scale when needed.</p>
<h3
id="what-are-the-common-challenges-when-deploying-machine-learning-models-on-kubernetes">3.
What are the common challenges when deploying machine learning models on
Kubernetes?</h3>
<p>Some common challenges when we deploy machine learning models on
Kubernetes are managing model versions, resource management, keeping
high availability, and updating models without downtime. We can use
tools like Helm for package management. Monitoring tools like Prometheus
can help us solve these challenges. This way, we can have a better
deployment of TensorFlow Serving on Kubernetes.</p>
<h3
id="how-do-i-troubleshoot-issues-with-tensorflow-serving-on-kubernetes">4.
How do I troubleshoot issues with TensorFlow Serving on Kubernetes?</h3>
<p>To fix issues with TensorFlow Serving on Kubernetes, we should first
check the logs of our TensorFlow Serving pods. We can do this using
<code>kubectl logs</code>. We also need to make sure our Kubernetes
resources are set up correctly. This includes service definitions and
ingress rules. Tools like kubectl port-forward can help us test our API
locally. Monitoring tools can give us information about how our
deployment is performing.</p>
<h3 id="can-i-use-gpus-for-tensorflow-serving-on-kubernetes">5. Can I
use GPUs for TensorFlow Serving on Kubernetes?</h3>
<p>Yes, we can use GPUs for TensorFlow Serving on Kubernetes. This helps
speed up inference for our machine learning models. To do this, we need
to make sure our Kubernetes cluster can support GPU scheduling. We will
have to specify GPU resource requests in our pod specs. Also, we should
use a GPU-enabled Docker image for TensorFlow Serving. This setup helps
improve performance for models that need a lot of resources.</p>
<p>For more info on deploying machine learning models with Kubernetes
and using TensorFlow Serving, check these resources: <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-deploy-machine-learning-models-on-kubernetes.html">How
Do I Deploy Machine Learning Models on Kubernetes?</a> and <a
href="https://bestonlinetutorial.com/kubernetes/how-do-i-manage-gpus-in-kubernetes.html">How
Do I Manage GPUs in Kubernetes?</a>.</p>

                        </div>

                    </div>
                    <!--//container-->
                </article>

            </div>
            <!--//main-wrapper-->

            <div id="footer-placeholder"></div>

            <!-- Javascript -->
            <script src="/assets/plugins/popper.min.js" defer></script>
            <script src="/assets/plugins/bootstrap/js/bootstrap.min.js" defer></script>
            <script src="/assets/fontawesome/js/all.min.js" defer></script>
        </body>

        </html>
            
            