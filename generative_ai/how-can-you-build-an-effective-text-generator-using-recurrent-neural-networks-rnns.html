
    
            <!DOCTYPE html>
        <html lang="en">

        <head>
            <!-- Google tag (gtag.js) -->
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-TFCQEJR7TD"></script>
            <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-TFCQEJR7TD');
            </script>
            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
<script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "name": "BestOnlineTutorial",
      "url": "https://www.bestonlinetutorial.com/"
    }
    </script>
            <!-- Common CSS & Icons -->
            <link rel="icon" href="/favicon.ico" type="image/x-icon">
            <link rel="stylesheet" href="/assets/plugins/highlight/styles/monokai-sublime.css">
            <link id="theme-style" rel="stylesheet" href="/assets/css/theme-8.css">
            <link rel="stylesheet" href="/assets/css/post.css">
            <title>How Can You Build an Effective Text Generator Using Recurrent Neural Networks (RNNs)?</title>
            <meta name="description" content="Discover how to build an effective text generator using RNNs. Explore techniques, tips, and best practices for optimal results!">
            <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
	        <script src="/assets/js/blog.js"></script>
        </head>

        <body>

            <div id="header-placeholder"></div>

            <div class="main-wrapper">

                <article class="blog-post px-3 py-5 p-md-5">
                    <div class="container single-col-max-width">
                        <header class="blog-post-header">
                            <h1 class="title mb-2">How Can You Build an Effective Text Generator Using Recurrent Neural Networks (RNNs)?</h1>
                        </header>

                        <div class="blog-post-body">
                            <p>Building a good text generator with Recurrent Neural Networks (RNNs)
is about using RNNs to understand and guess text sequences. RNNs help us
find patterns in data that comes in order. This makes them great for
text generation. The words we used before can change what words come
next.</p>
<p>In this article, we will look at the steps we need to make a strong
text generator with RNNs. First, we will explain the basics of RNNs.
Then, we will set up our tools for RNN text generation. After that, we
will prepare our dataset and design our RNN structure. Finally, we will
implement the RNN model. We will also talk about how to train our RNN
model and check how well it works. We will share practical examples of
text generation with RNNs. Here are the topics we will cover:</p>
<ul>
<li>How to Build a Good Text Generator Using Recurrent Neural Networks
RNNs</li>
<li>Understanding the Basics of Recurrent Neural Networks RNNs</li>
<li>Setting Up Our Tools for RNN Text Generation</li>
<li>Preparing Our Dataset for RNN Text Generation</li>
<li>Designing Our RNN Structure for Text Generation</li>
<li>Implementing the RNN Model for Text Generation</li>
<li>Training Our RNN Model for Good Text Generation</li>
<li>Checking the Performance of Our RNN Text Generator</li>
<li>Practical Examples of Text Generation with RNNs</li>
<li>Common Questions</li>
</ul>
<p>For more information and related topics, we can check these articles:
<a
href="https://bestonlinetutorial.com/generative_ai/what-is-generative-ai-and-how-does-it-work-a-comprehensive-guide.html">What
is Generative AI and How Does it Work?</a> and <a
href="https://bestonlinetutorial.com/generative_ai/how-do-neural-networks-fuel-the-capabilities-of-generative-ai.html">How
Do Neural Networks Help the Capabilities of Generative AI?</a>.</p>
<h2
id="understanding-the-basics-of-recurrent-neural-networks-rnns">Understanding
the Basics of Recurrent Neural Networks (RNNs)</h2>
<p>Recurrent Neural Networks (RNNs) are a type of neural networks. They
help us process data that comes in a sequence. Unlike regular
feedforward neural networks, RNNs have connections that loop back. This
lets them keep a hidden state. The hidden state remembers information
from earlier inputs. This is why RNNs work well for tasks like
generating text, modeling language, and predicting time series.</p>
<h3 id="key-concepts-of-rnns">Key Concepts of RNNs:</h3>
<ul>
<li><p><strong>Hidden State:</strong> RNNs have a hidden state vector (
h_t ). We update it at each time step ( t ) using the current input (
x_t ) and the last hidden state ( h_{t-1} ): [ h_t = f(W_h h_{t-1} + W_x
x_t + b) ] Here, ( f ) is a function we use to add non-linearity. We can
use functions like tanh or ReLU. ( W_h ) is the weight matrix for the
hidden state. ( W_x ) is the matrix for the input. ( b ) is the
bias.</p></li>
<li><p><strong>Output Layer:</strong> The output at each time step is: [
y_t = W_y h_t + b_y ] In this case, ( W_y ) is the output weight matrix
and ( b_y ) is the bias for the output.</p></li>
<li><p><strong>Training RNNs:</strong> We train RNNs using a method
called Backpropagation Through Time (BPTT). This means we unfold the RNN
over time steps and then apply backpropagation.</p></li>
<li><p><strong>Vanishing Gradient Problem:</strong> RNNs can face the
vanishing gradient problem. This makes it hard to learn long-term
dependencies. We can solve this problem using special architectures like
LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit).</p></li>
</ul>
<h3 id="use-cases">Use Cases:</h3>
<ul>
<li><strong>Text Generation:</strong> We can use RNNs to create text by
guessing the next character or word based on what came before.</li>
<li><strong>Language Translation:</strong> RNNs are also good for
translating text from one language to another in sequence-to-sequence
models.</li>
<li><strong>Speech Recognition:</strong> RNNs work well for tasks that
involve sequences over time, like audio signals.</li>
</ul>
<p>By learning these basic ideas, we can use RNNs for many applications.
They are especially useful for building a good text generator. If you
want to know more about generative AI, you can check this <a
href="https://bestonlinetutorial.com/generative_ai/what-is-generative-ai-and-how-does-it-work-a-comprehensive-guide.html">comprehensive
guide on generative AI</a>.</p>
<h2 id="setting-up-your-environment-for-rnn-text-generation">Setting Up
Your Environment for RNN Text Generation</h2>
<p>To make a good text generator using Recurrent Neural Networks (RNNs),
we need to set up our development environment right. Here are the steps
to help us have a smooth setup for RNN text generation.</p>
<ol type="1">
<li><p><strong>Install Required Libraries</strong>: We use Python and
need to install some libraries. The main libraries are TensorFlow (or
PyTorch), NumPy, and Matplotlib for making graphs. We can install these
with pip:</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tensorflow numpy matplotlib</span></code></pre></div></li>
<li><p><strong>Set Up Jupyter Notebook (Optional)</strong>: For a fun
coding experience, we can use Jupyter Notebook. If we want to use it, we
install it via pip:</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install jupyter</span></code></pre></div>
<p>After that, we start it by running:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">jupyter</span> notebook</span></code></pre></div></li>
<li><p><strong>Choose the Right Environment</strong>: We should use a
virtual environment to manage our tools. We can create a virtual
environment using <code>venv</code>:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> <span class="at">-m</span> venv rnn_env</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> rnn_env/bin/activate  <span class="co"># On Windows use: rnn_env\Scripts\activate</span></span></code></pre></div></li>
<li><p><strong>GPU Support</strong>: If we work with big datasets or
tricky models, we should think about using GPU support. We can install
the GPU version of TensorFlow if we have a compatible NVIDIA GPU:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode bash"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install tensorflow-gpu</span></code></pre></div></li>
<li><p><strong>Verify Installation</strong>: It is good to check that
TensorFlow is installed and can use our GPU if we have one. We can check
this with the Python code below:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Num GPUs Available: &quot;</span>, <span class="bu">len</span>(tf.config.list_physical_devices(<span class="st">&#39;GPU&#39;</span>)))</span></code></pre></div></li>
<li><p><strong>Set Up Data Files</strong>: We need to have our text
files ready to train our RNN. This could mean downloading datasets or
making our own text data for training.</p></li>
<li><p><strong>IDE Configuration</strong>: We can use any IDE like
PyCharm, VS Code, or Jupyter Notebook for coding. We need to make sure
our IDE knows about the virtual environment with our installed
libraries.</p></li>
</ol>
<p>By following these steps, we will have our environment ready to build
a good text generator using RNNs. This setup helps us to make the
development process easier and use the power of RNNs well. For more
information about generative AI and what it can do, we can check <a
href="https://bestonlinetutorial.com/generative_ai/what-is-generative-ai-and-how-does-it-work-a-comprehensive-guide.html">this
guide</a>.</p>
<h2 id="preparing-your-dataset-for-rnn-text-generation">Preparing Your
Dataset for RNN Text Generation</h2>
<p>To build a good text generator using Recurrent Neural Networks
(RNNs), we need to prepare our dataset well. This includes a few
important steps:</p>
<ol type="1">
<li><p><strong>Collecting Text Data</strong>: We should gather a big
collection of text related to the area where we want to create text.
This can be books, articles, or any text that meets our needs.</p></li>
<li><p><strong>Text Cleaning</strong>: We need to process the text to
remove any unwanted characters, symbols, and formatting problems. We can
use Python’s <code>re</code> module for regular expressions.</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text):</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r&#39;[^a-zA-Z0-9\s]&#39;</span>, <span class="st">&#39;&#39;</span>, text)  <span class="co"># Remove punctuation</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()  <span class="co"># Convert to lowercase</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text</span></code></pre></div></li>
<li><p><strong>Tokenization</strong>: We break the text into tokens like
words or characters. For RNNs, we often use character-level
tokenization. But sometimes word-level can also work based on what we
need.</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_text(texts):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    tokenizer <span class="op">=</span> Tokenizer(char_level<span class="op">=</span><span class="va">True</span>)  <span class="co"># Set char_level=False for word-level</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    tokenizer.fit_on_texts(texts)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer.texts_to_sequences(texts), tokenizer</span></code></pre></div></li>
<li><p><strong>Sequence Creation</strong>: We convert the tokenized data
into sequences of a fixed length. This is important for RNN input.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_sequences(data, seq_length):</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    sequences <span class="op">=</span> []</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data) <span class="op">-</span> seq_length):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>        seq <span class="op">=</span> data[i:i <span class="op">+</span> seq_length]</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        sequences.append(seq)</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(sequences)</span></code></pre></div></li>
<li><p><strong>Splitting Dataset</strong>: We should divide our dataset
into training, validation, and test sets. This helps us check how well
our RNN model performs.</p>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_dataset(data):</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    train_data, test_data <span class="op">=</span> train_test_split(data, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_data, test_data</span></code></pre></div></li>
<li><p><strong>Normalization</strong>: We may need to normalize the
dataset, especially if we use numbers along with text data.</p></li>
<li><p><strong>Saving the Processed Data</strong>: We save the processed
sequences and mappings for later use in training the model.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> save_data(data, filename):</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> f:</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        pickle.dump(data, f)</span></code></pre></div></li>
</ol>
<p>Preparing the dataset is very important for making a successful RNN
text generator. If we want more details on how to start with generative
AI, we can look at this <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-steps-to-get-started-with-generative-ai-a-beginners-guide.html">beginner’s
guide</a>.</p>
<h2 id="designing-your-rnn-architecture-for-text-generation">Designing
Your RNN Architecture for Text Generation</h2>
<p>When we design a good Recurrent Neural Network (RNN) for text
generation, we need to think about some important parts. This helps us
get better performance and create clear output.</p>
<ol type="1">
<li><strong>Input Representation</strong>:
<ul>
<li>We can use word embeddings like Word2Vec or GloVe to change words
into numbers. This helps us keep the meaning of the words.</li>
<li>Here is a simple code to create embeddings with Keras:</li>
</ul>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(corpus)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> tokenizer.texts_to_sequences(corpus)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>padded_sequences <span class="op">=</span> pad_sequences(sequences, maxlen<span class="op">=</span>max_length)</span></code></pre></div></li>
<li><strong>RNN Layers</strong>:
<ul>
<li>We can pick different types of RNNs. LSTM (Long Short-Term Memory)
or GRU (Gated Recurrent Unit) are popular. They can remember information
over long sequences.</li>
<li>Here is an example of a simple architecture:</li>
</ul>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> LSTM, Dense, Embedding</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>model.add(Embedding(input_dim<span class="op">=</span>vocab_size, output_dim<span class="op">=</span>embedding_dim, input_length<span class="op">=</span>max_length))</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>model.add(LSTM(units<span class="op">=</span><span class="dv">128</span>, return_sequences<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>model.add(LSTM(units<span class="op">=</span><span class="dv">128</span>))</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>model.add(Dense(units<span class="op">=</span>vocab_size, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>))</span></code></pre></div></li>
<li><strong>Regularization Techniques</strong>:
<ul>
<li>We can add dropout layers. This helps to stop overfitting.</li>
<li>Here is how we can do it:</li>
</ul>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Dropout</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>model.add(Dropout(<span class="fl">0.2</span>))</span></code></pre></div></li>
<li><strong>Output Layer</strong>:
<ul>
<li>We need a Dense layer with softmax activation to guess the next word
in the sequence.</li>
<li>It is important that the output size is the same as the vocabulary
size.</li>
</ul></li>
<li><strong>Loss Function and Optimizer</strong>:
<ul>
<li>A common loss function we use is categorical crossentropy.</li>
<li>The Adam optimizer works well for training RNNs.</li>
</ul>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div></li>
<li><strong>Hyperparameter Tuning</strong>:
<ul>
<li>We should try different numbers of LSTM units, dropout rates, and
learning rates. This helps us find the best setup for our dataset.</li>
</ul></li>
<li><strong>Sequence Length</strong>:
<ul>
<li>It is good to use a fixed maximum sequence length for input. This
keeps things consistent during training and generation.</li>
</ul></li>
<li><strong>Batch Size</strong>:
<ul>
<li>We need to pick a good batch size. Smaller sizes can give better
generalization but can slow down training.</li>
</ul></li>
<li><strong>Training Configuration</strong>:
<ul>
<li>We can use callbacks like ModelCheckpoint. This saves the best model
during training based on validation loss.</li>
</ul></li>
</ol>
<p>By designing our RNN architecture with these points, we can make a
good text generator. It will produce clear and relevant text. For more
tips on how to generate text using RNNs, check out <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-steps-to-get-started-with-generative-ai-a-beginners-guide.html">this
guide</a>.</p>
<h2 id="implementing-the-rnn-model-for-text-generation">Implementing the
RNN Model for Text Generation</h2>
<p>We can create a text generator using Recurrent Neural Networks
(RNNs). We usually use frameworks like TensorFlow or PyTorch. Here is a
simple guide for coding the RNN model to generate text.</p>
<h3 id="step-1-import-required-libraries">Step 1: Import Required
Libraries</h3>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Embedding, SimpleRNN, Dense</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span></code></pre></div>
<h3 id="step-2-prepare-your-dataset">Step 2: Prepare Your Dataset</h3>
<p>Let’s say we have some text data:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example text data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">&quot;Your text data goes here. This could be multiple sentences.&quot;</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenization</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts([text])</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>total_words <span class="op">=</span> <span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Creating input sequences and labels</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>input_sequences <span class="op">=</span> []</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(tokenizer.texts_to_sequences([text])[<span class="dv">0</span>])):</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    n_gram_sequence <span class="op">=</span> tokenizer.texts_to_sequences([text])[<span class="dv">0</span>][:i <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    input_sequences.append(n_gram_sequence)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Padding sequences</span></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>max_sequence_length <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> input_sequences)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>input_sequences <span class="op">=</span> pad_sequences(input_sequences, maxlen<span class="op">=</span>max_sequence_length, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> input_sequences[:, :<span class="op">-</span><span class="dv">1</span>], input_sequences[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> tf.keras.utils.to_categorical(y, num_classes<span class="op">=</span>total_words)</span></code></pre></div>
<h3 id="step-3-design-the-rnn-architecture">Step 3: Design the RNN
Architecture</h3>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>model.add(Embedding(total_words, <span class="dv">100</span>, input_length<span class="op">=</span>max_sequence_length<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>model.add(SimpleRNN(<span class="dv">100</span>))  <span class="co"># We can also try LSTM or GRU</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>model.add(Dense(total_words, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>))</span></code></pre></div>
<h3 id="step-4-compile-the-model">Step 4: Compile the Model</h3>
<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div>
<h3 id="step-5-train-the-model">Step 5: Train the Model</h3>
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model.fit(X, y, epochs<span class="op">=</span><span class="dv">100</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
<h3 id="step-6-generate-text">Step 6: Generate Text</h3>
<p>To make text from the trained model, we can use this function:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(seed_text, next_words, model, tokenizer, max_sequence_length):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(next_words):</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> tokenizer.texts_to_sequences([seed_text])[<span class="dv">0</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> pad_sequences([token_list], maxlen<span class="op">=</span>max_sequence_length<span class="op">-</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        predicted <span class="op">=</span> model.predict(token_list, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>        predicted_word_index <span class="op">=</span> np.argmax(predicted, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        output_word <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, index <span class="kw">in</span> tokenizer.word_index.items():</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index <span class="op">==</span> predicted_word_index:</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>                output_word <span class="op">=</span> word</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        seed_text <span class="op">+=</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> output_word</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seed_text</span></code></pre></div>
<h3 id="example-usage">Example Usage</h3>
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_text(<span class="st">&quot;Your seed text&quot;</span>, <span class="dv">5</span>, model, tokenizer, max_sequence_length))</span></code></pre></div>
<p>This code gives us a simple way to implement an RNN model for text
generation. We can look for more complex setups. For example, we can use
LSTM or GRU layers to get better results. For more information on RNNs
and text generation, check out <a
href="https://bestonlinetutorial.com/generative_ai/how-do-neural-networks-fuel-the-capabilities-of-generative-ai.html">this
resource on neural networks in generative AI</a>.</p>
<h2 id="training-our-rnn-model-for-effective-text-generation">Training
Our RNN Model for Effective Text Generation</h2>
<p>To train our Recurrent Neural Network (RNN) model for good text
generation, we can follow these simple steps:</p>
<ol type="1">
<li><p><strong>Prepare Our Data</strong>: We need to clean and
preprocess our dataset. We should tokenize the text and change the
tokens into sequences.</p>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example text data</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">&quot;Hello world&quot;</span>, <span class="st">&quot;How are you&quot;</span>, <span class="st">&quot;Hello RNN&quot;</span>]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenization</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(texts)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>total_words <span class="op">=</span> <span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Create input sequences</span></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>input_sequences <span class="op">=</span> []</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> line <span class="kw">in</span> texts:</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    token_list <span class="op">=</span> tokenizer.texts_to_sequences([line])[<span class="dv">0</span>]</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(token_list)):</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>        n_gram_sequence <span class="op">=</span> token_list[:i <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        input_sequences.append(n_gram_sequence)</span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Pad sequences</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>max_sequence_length <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> input_sequences)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>input_sequences <span class="op">=</span> pad_sequences(input_sequences, maxlen<span class="op">=</span>max_sequence_length, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span></code></pre></div></li>
<li><p><strong>Define Features and Labels</strong>: We need to split the
input sequences into features (X) and labels (y).</p>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>input_sequences <span class="op">=</span> np.array(input_sequences)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> input_sequences[:, :<span class="op">-</span><span class="dv">1</span>], input_sequences[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.eye(total_words)[y]  <span class="co"># One-hot encoding of labels</span></span></code></pre></div></li>
<li><p><strong>Build Our RNN Model</strong>: Next, we will create our
RNN model using Keras.</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> Embedding, SimpleRNN, Dense</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>model.add(Embedding(total_words, <span class="dv">50</span>, input_length<span class="op">=</span>max_sequence_length <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>model.add(SimpleRNN(<span class="dv">100</span>, return_sequences<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>model.add(Dense(total_words, activation<span class="op">=</span><span class="st">&#39;softmax&#39;</span>))</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div></li>
<li><p><strong>Train the Model</strong>: We will use the model’s
<code>fit</code> method to train our RNN.</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>model.fit(X, y, epochs<span class="op">=</span><span class="dv">100</span>, verbose<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div></li>
<li><p><strong>Monitor Training</strong>: We can use callbacks like
<code>ModelCheckpoint</code> and <code>EarlyStopping</code> to save the
best model and stop overfitting.</p>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.callbacks <span class="im">import</span> ModelCheckpoint, EarlyStopping</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> ModelCheckpoint(<span class="st">&#39;rnn_text_generator.h5&#39;</span>, save_best_only<span class="op">=</span><span class="va">True</span>, monitor<span class="op">=</span><span class="st">&#39;loss&#39;</span>, mode<span class="op">=</span><span class="st">&#39;min&#39;</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>early_stopping <span class="op">=</span> EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;loss&#39;</span>, patience<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>model.fit(X, y, epochs<span class="op">=</span><span class="dv">100</span>, verbose<span class="op">=</span><span class="dv">1</span>, callbacks<span class="op">=</span>[checkpoint, early_stopping])</span></code></pre></div></li>
<li><p><strong>Generate Text</strong>: After we finish training, we can
use the model to make text based on a starting input.</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(seed_text, next_words, model, max_sequence_length):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(next_words):</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> tokenizer.texts_to_sequences([seed_text])[<span class="dv">0</span>]</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> pad_sequences([token_list], maxlen<span class="op">=</span>max_sequence_length<span class="op">-</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        predicted <span class="op">=</span> model.predict(token_list, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        predicted_word_index <span class="op">=</span> np.argmax(predicted, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        output_word <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, index <span class="kw">in</span> tokenizer.word_index.items():</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index <span class="op">==</span> predicted_word_index:</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>                output_word <span class="op">=</span> word</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>        seed_text <span class="op">+=</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> output_word</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seed_text</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_text(<span class="st">&quot;Hello&quot;</span>, <span class="dv">5</span>, model, max_sequence_length))</span></code></pre></div></li>
</ol>
<p>By using this way, we can train an RNN model for text generation. It
helps us create text that is clear and makes sense. For more about how
to set up our RNN for text generation, we can check out <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-steps-to-get-started-with-generative-ai-a-beginners-guide.html">this
guide on generative AI</a>.</p>
<h2
id="evaluating-the-performance-of-your-rnn-text-generator">Evaluating
the Performance of Your RNN Text Generator</h2>
<p>We need to evaluate the performance of our RNN text generator. This
helps us make sure it creates clear and relevant text. Here are some
important metrics and methods to check how well our model works:</p>
<ol type="1">
<li><p><strong>Perplexity</strong>: This is a common way to see how good
a model is at predicting a sample. Lower perplexity means better
performance.</p>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_perplexity(log_probs):</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>np.mean(log_probs))</span></code></pre></div></li>
<li><p><strong>BLEU Score</strong>: This score shows how similar the
generated text is to the reference text. A higher BLEU score means
better quality.</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.translate.bleu_score <span class="im">import</span> sentence_bleu</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>reference <span class="op">=</span> [[<span class="st">&#39;this&#39;</span>, <span class="st">&#39;is&#39;</span>, <span class="st">&#39;a&#39;</span>, <span class="st">&#39;test&#39;</span>]]</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>candidate <span class="op">=</span> [<span class="st">&#39;this&#39;</span>, <span class="st">&#39;is&#39;</span>, <span class="st">&#39;test&#39;</span>]</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>score <span class="op">=</span> sentence_bleu(reference, candidate)</span></code></pre></div></li>
<li><p><strong>ROUGE Score</strong>: We can use this score to check text
summarization. It looks at the overlap of n-grams between the generated
text and the reference.</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> rouge <span class="im">import</span> Rouge</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>rouge <span class="op">=</span> Rouge()</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> rouge.get_scores(candidate, reference[<span class="dv">0</span>])</span></code></pre></div></li>
<li><p><strong>Human Evaluation</strong>: We can ask people to rate the
text quality. They can look at fluency, coherence, and relevance. This
gives insights that numbers alone may not show.</p></li>
<li><p><strong>Training Loss and Validation Loss</strong>: We should
watch the loss during training. This helps us know if the model is
learning well and not getting too used to the training data.</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>plt.plot(train_loss, label<span class="op">=</span><span class="st">&#39;Training Loss&#39;</span>)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plt.plot(val_loss, label<span class="op">=</span><span class="st">&#39;Validation Loss&#39;</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Epochs&#39;</span>)</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Loss&#39;</span>)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div></li>
<li><p><strong>Sample Generation</strong>: We can create a few samples
from the model and check their quality ourselves. This helps us see how
well the model understands context and creates meaningful text.</p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(model, seed_text, next_words<span class="op">=</span><span class="dv">50</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(next_words):</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> tokenizer.texts_to_sequences([seed_text])[<span class="dv">0</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> pad_sequences([token_list], maxlen<span class="op">=</span>max_sequence_length<span class="op">-</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        predicted <span class="op">=</span> model.predict(token_list, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>        output_word <span class="op">=</span> tokenizer.index_word[np.argmax(predicted)]</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>        seed_text <span class="op">+=</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> output_word</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seed_text</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>generated_text <span class="op">=</span> generate_text(trained_model, <span class="st">&quot;Once upon a time&quot;</span>)</span></code></pre></div></li>
</ol>
<p>We should use these methods to understand how well our RNN text
generator works. This will help us improve it further. For more ideas on
model evaluation, we can check <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-steps-to-get-started-with-generative-ai-a-beginners-guide.html">what
are the steps to get started with generative AI</a>.</p>
<h2 id="practical-examples-of-text-generation-using-rnns">Practical
Examples of Text Generation Using RNNs</h2>
<p>Recurrent Neural Networks (RNNs) are great tools for creating text.
They can keep track of context in sequences. Here are some simple
examples to show how we can make text using RNNs.</p>
<h3 id="example-1-character-level-text-generation">Example 1:
Character-Level Text Generation</h3>
<p>In this example, we will see how to create text at the character
level using an RNN in Keras.</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> LSTM, Dense, Embedding, Activation</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.utils <span class="im">import</span> to_categorical</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare dataset</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">&quot;Your training data text goes here.&quot;</span></span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>char_indices <span class="op">=</span> {c: i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>indices_char <span class="op">=</span> {i: c <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(chars)}</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>maxlen <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> []</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>next_chars <span class="op">=</span> []</span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(text) <span class="op">-</span> maxlen, step):</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>    sentences.append(text[i: i <span class="op">+</span> maxlen])</span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    next_chars.append(text[i <span class="op">+</span> maxlen])</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.zeros((<span class="bu">len</span>(sentences), maxlen, <span class="bu">len</span>(chars)), dtype<span class="op">=</span>np.<span class="bu">bool</span>)</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.zeros((<span class="bu">len</span>(sentences), <span class="bu">len</span>(chars)), dtype<span class="op">=</span>np.<span class="bu">bool</span>)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, sentence <span class="kw">in</span> <span class="bu">enumerate</span>(sentences):</span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t, char <span class="kw">in</span> <span class="bu">enumerate</span>(sentence):</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>        X[i, t, char_indices[char]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>    y[i, char_indices[next_chars[i]]] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Build RNN model</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>model.add(LSTM(<span class="dv">128</span>, input_shape<span class="op">=</span>(maxlen, <span class="bu">len</span>(chars))))</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>model.add(Dense(<span class="bu">len</span>(chars)))</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>model.add(Activation(<span class="st">&#39;softmax&#39;</span>))</span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>)</span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb34-38"><a href="#cb34-38" aria-hidden="true" tabindex="-1"></a>model.fit(X, y, batch_size<span class="op">=</span><span class="dv">128</span>, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb34-39"><a href="#cb34-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-40"><a href="#cb34-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate text</span></span>
<span id="cb34-41"><a href="#cb34-41" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_text(length<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb34-42"><a href="#cb34-42" aria-hidden="true" tabindex="-1"></a>    start_index <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="bu">len</span>(text) <span class="op">-</span> maxlen <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb34-43"><a href="#cb34-43" aria-hidden="true" tabindex="-1"></a>    generated <span class="op">=</span> <span class="st">&#39;&#39;</span></span>
<span id="cb34-44"><a href="#cb34-44" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> text[start_index: start_index <span class="op">+</span> maxlen]</span>
<span id="cb34-45"><a href="#cb34-45" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-46"><a href="#cb34-46" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(length):</span>
<span id="cb34-47"><a href="#cb34-47" aria-hidden="true" tabindex="-1"></a>        x_pred <span class="op">=</span> np.zeros((<span class="dv">1</span>, maxlen, <span class="bu">len</span>(chars)))</span>
<span id="cb34-48"><a href="#cb34-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t, char <span class="kw">in</span> <span class="bu">enumerate</span>(sentence):</span>
<span id="cb34-49"><a href="#cb34-49" aria-hidden="true" tabindex="-1"></a>            x_pred[<span class="dv">0</span>, t, char_indices[char]] <span class="op">=</span> <span class="fl">1.</span></span>
<span id="cb34-50"><a href="#cb34-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-51"><a href="#cb34-51" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model.predict(x_pred, verbose<span class="op">=</span><span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb34-52"><a href="#cb34-52" aria-hidden="true" tabindex="-1"></a>        next_index <span class="op">=</span> np.random.choice(<span class="bu">len</span>(chars), p<span class="op">=</span>preds)</span>
<span id="cb34-53"><a href="#cb34-53" aria-hidden="true" tabindex="-1"></a>        next_char <span class="op">=</span> indices_char[next_index]</span>
<span id="cb34-54"><a href="#cb34-54" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-55"><a href="#cb34-55" aria-hidden="true" tabindex="-1"></a>        generated <span class="op">+=</span> next_char</span>
<span id="cb34-56"><a href="#cb34-56" aria-hidden="true" tabindex="-1"></a>        sentence <span class="op">=</span> sentence[<span class="dv">1</span>:] <span class="op">+</span> next_char</span>
<span id="cb34-57"><a href="#cb34-57" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb34-58"><a href="#cb34-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generated</span>
<span id="cb34-59"><a href="#cb34-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-60"><a href="#cb34-60" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_text(<span class="dv">200</span>))</span></code></pre></div>
<h3 id="example-2-word-level-text-generation">Example 2: Word-Level Text
Generation</h3>
<p>Now we will see how to create text at the word level using an
RNN.</p>
<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.models <span class="im">import</span> Sequential</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.layers <span class="im">import</span> LSTM, Dense, Embedding, Activation</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.text <span class="im">import</span> Tokenizer</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> keras.preprocessing.sequence <span class="im">import</span> pad_sequences</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare dataset</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [<span class="st">&quot;Your training sentences go here.&quot;</span>, <span class="st">&quot;Another training sentence.&quot;</span>]</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> Tokenizer()</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>tokenizer.fit_on_texts(texts)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>sequences <span class="op">=</span> tokenizer.texts_to_sequences(texts)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> []</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> []</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> seq <span class="kw">in</span> sequences:</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(seq)):</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        X.append(seq[:i])</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        y.append(seq[i])</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>maxlen <span class="op">=</span> <span class="bu">max</span>(<span class="bu">len</span>(x) <span class="cf">for</span> x <span class="kw">in</span> X)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pad_sequences(X, maxlen<span class="op">=</span>maxlen)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> to_categorical(y, num_classes<span class="op">=</span><span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Build RNN model</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Sequential()</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>model.add(Embedding(<span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span>, <span class="dv">50</span>, input_length<span class="op">=</span>maxlen))</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>model.add(LSTM(<span class="dv">128</span>))</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>model.add(Dense(<span class="bu">len</span>(tokenizer.word_index) <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>model.add(Activation(<span class="st">&#39;softmax&#39;</span>))</span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;categorical_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>)</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>model.fit(X, y, batch_size<span class="op">=</span><span class="dv">128</span>, epochs<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate text</span></span>
<span id="cb35-39"><a href="#cb35-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_word_sequence(seed_text, next_words<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb35-40"><a href="#cb35-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(next_words):</span>
<span id="cb35-41"><a href="#cb35-41" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> tokenizer.texts_to_sequences([seed_text])[<span class="dv">0</span>]</span>
<span id="cb35-42"><a href="#cb35-42" aria-hidden="true" tabindex="-1"></a>        token_list <span class="op">=</span> pad_sequences([token_list], maxlen<span class="op">=</span>maxlen<span class="op">-</span><span class="dv">1</span>, padding<span class="op">=</span><span class="st">&#39;pre&#39;</span>)</span>
<span id="cb35-43"><a href="#cb35-43" aria-hidden="true" tabindex="-1"></a>        predicted <span class="op">=</span> model.predict(token_list, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb35-44"><a href="#cb35-44" aria-hidden="true" tabindex="-1"></a>        classes <span class="op">=</span> np.argmax(predicted, axis<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb35-45"><a href="#cb35-45" aria-hidden="true" tabindex="-1"></a>        output_word <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb35-46"><a href="#cb35-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-47"><a href="#cb35-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> word, index <span class="kw">in</span> tokenizer.word_index.items():</span>
<span id="cb35-48"><a href="#cb35-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> index <span class="op">==</span> classes:</span>
<span id="cb35-49"><a href="#cb35-49" aria-hidden="true" tabindex="-1"></a>                output_word <span class="op">=</span> word</span>
<span id="cb35-50"><a href="#cb35-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb35-51"><a href="#cb35-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb35-52"><a href="#cb35-52" aria-hidden="true" tabindex="-1"></a>        seed_text <span class="op">+=</span> <span class="st">&quot; &quot;</span> <span class="op">+</span> output_word</span>
<span id="cb35-53"><a href="#cb35-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb35-54"><a href="#cb35-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> seed_text</span>
<span id="cb35-55"><a href="#cb35-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-56"><a href="#cb35-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(generate_word_sequence(<span class="st">&quot;Your seed text here&quot;</span>, next_words<span class="op">=</span><span class="dv">10</span>))</span></code></pre></div>
<p>These examples show how we can do character-level and word-level text
generation using RNNs. We can change the dataset and model settings to
meet our needs. For more information on generative models and how they
work, check out <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-real-life-applications-of-generative-ai.html">this
guide on generative AI</a>.</p>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<h3
id="what-are-recurrent-neural-networks-rnns-and-how-do-they-work-for-text-generation">1.
What are Recurrent Neural Networks (RNNs) and how do they work for text
generation?</h3>
<p>Recurrent Neural Networks, or RNNs, are a type of neural network.
They work well for sequence data. This makes them great for tasks like
text generation. RNNs use memory cells. These cells keep information
from previous inputs. Because of this, RNNs can create coherent text.
They predict the next word based on the previous words. This design
helps RNNs create text that sounds like humans wrote it.</p>
<h3 id="what-environment-is-best-for-building-an-rnn-text-generator">2.
What environment is best for building an RNN text generator?</h3>
<p>To build a good text generator with RNNs, we should use a Python
environment. Libraries like TensorFlow or PyTorch are helpful. Jupyter
Notebook is also a good choice for coding and seeing results right away.
It is important to have access to a GPU. This makes training faster,
especially with big datasets. This setup helps us develop and train our
RNN model better.</p>
<h3 id="how-do-i-prepare-my-dataset-for-rnn-text-generation">3. How do I
prepare my dataset for RNN text generation?</h3>
<p>Preparing our dataset for RNN text generation means cleaning and
getting the text ready. We should remove any unneeded characters. Then
we need to tokenize the text into words or smaller parts. Next, we
convert these tokens into numbers. Lowercasing the text, removing stop
words, and fixing special characters can help our RNN model work
better.</p>
<h3
id="what-are-some-common-architectures-used-for-rnn-text-generation">4.
What are some common architectures used for RNN text generation?</h3>
<p>Some common architectures for RNN text generation are Long Short-Term
Memory (LSTM) networks and Gated Recurrent Units (GRUs). These
architectures solve the vanishing gradient problem from regular RNNs.
This helps them learn long-range relationships in text. We need to
choose the right architecture based on how complex our task is and how
much training data we have.</p>
<h3 id="how-can-i-evaluate-the-performance-of-my-rnn-text-generator">5.
How can I evaluate the performance of my RNN text generator?</h3>
<p>We can evaluate the performance of our RNN text generator using some
metrics. These include perplexity, BLEU score, or human evaluation.
Perplexity shows how well the model predicts a sample. The BLEU score
compares the generated text to reference text. Doing user studies can
also give us good feedback on how coherent and creative the text is.</p>
<p>For more insights on generative AI and its uses, we can check
articles like <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-real-life-applications-of-generative-ai.html">What
are the real-life applications of generative AI?</a> and <a
href="https://bestonlinetutorial.com/generative_ai/how-do-neural-networks-fuel-the-capabilities-of-generative-ai.html">How
do neural networks fuel the capabilities of generative AI?</a>. These
resources help us understand RNNs and their role in generating text.</p>

                        </div>

                    </div>
                    <!--//container-->
                </article>

            </div>
            <!--//main-wrapper-->

            <div id="footer-placeholder"></div>

            <!-- Javascript -->
            <script src="/assets/plugins/popper.min.js" defer></script>
            <script src="/assets/plugins/bootstrap/js/bootstrap.min.js" defer></script>
            <script src="/assets/fontawesome/js/all.min.js" defer></script>
        </body>

        </html>
            
            