
    
            <!DOCTYPE html>
        <html lang="en">

        <head>
            <!-- Google tag (gtag.js) -->
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-TFCQEJR7TD"></script>
            <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-TFCQEJR7TD');
            </script>
            <meta charset="utf-8">
            <meta http-equiv="X-UA-Compatible" content="IE=edge">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <!-- Common CSS & Icons -->
            <link rel="icon" href="/favicon.ico" type="image/x-icon">
            <link rel="stylesheet" href="/assets/plugins/highlight/styles/monokai-sublime.css">
            <link id="theme-style" rel="stylesheet" href="/assets/css/theme-8.css">
            <link rel="stylesheet" href="/assets/css/post.css">
            <title>What Are the Mathematical Foundations of Generative Models?</title>
            <meta name="description" content="Explore the mathematical foundations of generative models and discover their impact on AI and machine learning in this insightful blog.">
            <script src="https://code.jquery.com/jquery-3.6.0.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
	        <script src="/assets/js/blog.js"></script>
        </head>

        <body>

            <div id="header-placeholder"></div>

            <div class="main-wrapper">

                <article class="blog-post px-3 py-5 p-md-5">
                    <div class="container single-col-max-width">
                        <header class="blog-post-header">
                            <h1 class="title mb-2">What Are the Mathematical Foundations of Generative Models?</h1>
                        </header>

                        <div class="blog-post-body">
                            <p>Generative models are a type of statistical models. They help us
create new data points based on patterns we learn from training data.
These models understand the joint probability distribution of the input
data. This lets them create new samples that look like the original
dataset. The math behind generative models includes concepts like
probability theory, latent variables, and optimization techniques. These
ideas are important for us to know how these models work and learn from
data.</p>
<p>In this article, we will look into the math behind generative models
in machine learning. We will see why probability distributions matter.
We will also talk about latent variables and the math behind techniques
like Generative Adversarial Networks (GANs) and Variational Inference.
We will explain the ideas behind autoencoders. We will give practical
examples of generative models in Python. Plus, we will discuss how these
models learn from data. The main sections we will cover include:</p>
<ul>
<li>What Are the Mathematical Foundations of Generative Models in
Machine Learning?</li>
<li>Understanding Probability Distributions in Generative Models</li>
<li>The Role of Latent Variables in Generative Models</li>
<li>Exploring Generative Adversarial Networks and Their Mathematics</li>
<li>An Overview of Variational Inference in Generative Models</li>
<li>Mathematical Principles Behind Autoencoders in Generative
Models</li>
<li>Practical Examples of Generative Models in Python</li>
<li>How Do Generative Models Learn from Data?</li>
<li>Frequently Asked Questions</li>
</ul>
<p>For more information on generative AI, you can check the guide on <a
href="https://bestonlinetutorial.com/generative_ai/what-is-generative-ai-and-how-does-it-work-a-comprehensive-guide.html">what
generative AI is and how it works</a> or learn about the <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-key-differences-between-generative-and-discriminative-models-understanding-their-unique-features-and-applications.html">key
differences between generative and discriminative models</a>.</p>
<h2
id="understanding-probability-distributions-in-generative-models">Understanding
Probability Distributions in Generative Models</h2>
<p>Probability distributions are very important for generative models in
machine learning. They show how the chance of a random variable is
spread over different values. In generative models, we use them to
describe the basic data distribution from which we get the training
data.</p>
<h3 id="key-concepts">Key Concepts</h3>
<ul>
<li><strong>Probability Density Function (PDF)</strong>: For continuous
variables, the PDF shows the chance of a random variable being a certain
value.</li>
<li><strong>Probability Mass Function (PMF)</strong>: For discrete
variables, the PMF gives the chance that a discrete random variable is
equal to a specific value.</li>
<li><strong>Cumulative Distribution Function (CDF)</strong>: The CDF
shows the chance that a random variable is less than or equal to a
certain number.</li>
</ul>
<h3 id="common-distributions-used-in-generative-models">Common
Distributions Used in Generative Models</h3>
<ol type="1">
<li><strong>Gaussian Distribution (Normal Distribution)</strong>:
<ul>
<li>It is defined by its mean (()) and standard deviation (()).</li>
<li>PDF: [ f(x) = e^{-} ]</li>
</ul></li>
<li><strong>Uniform Distribution</strong>:
<ul>
<li>All outcomes have the same chance within a certain range ([a,
b]).</li>
<li>PDF: [ f(x) =
]</li>
</ul></li>
<li><strong>Multinomial Distribution</strong>:
<ul>
<li>It is a general form of the binomial distribution for many
outcomes.</li>
<li>PMF: [ P(X_1 = k_1, X_2 = k_2, , X_n = k_n) = p_1^{k_1} p_2^{k_2}
p_n^{k_n} ] where (p_i) are the chances of each outcome.</li>
</ul></li>
</ol>
<h3 id="practical-implementation-in-python">Practical Implementation in
Python</h3>
<p>We can show how to use probability distributions in generative
models. We use libraries like <code>numpy</code> and
<code>matplotlib</code> to see a Gaussian distribution.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="fl">0.1</span>  <span class="co"># mean and standard deviation</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random data</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> np.random.normal(mu, sigma, <span class="dv">1000</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.hist(s, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, color<span class="op">=</span><span class="st">&#39;g&#39;</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Overlay the PDF</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>xmin, xmax <span class="op">=</span> plt.xlim()</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(xmin, xmax, <span class="dv">100</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(sigma <span class="op">*</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)) <span class="op">*</span> np.exp(<span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ((x <span class="op">-</span> mu) <span class="op">/</span> sigma) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>plt.plot(x, p, <span class="st">&#39;k&#39;</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Gaussian Distribution&#39;</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>This code makes a histogram of random samples from a Gaussian
distribution and adds the theoretical PDF on top. When we understand
these distributions, we can build models that catch and repeat the basic
data patterns. This is very important for good generative modeling.</p>
<p>For more details on the uses and differences between generative and
discriminative models, we can check <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-key-differences-between-generative-and-discriminative-models-understanding-their-unique-features-and-applications.html">this
guide</a>.</p>
<h2 id="the-role-of-latent-variables-in-generative-models">The Role of
Latent Variables in Generative Models</h2>
<p>Latent variables are important parts of generative models. They act
like hidden factors that help explain the data we see. In generative
modeling, these variables capture the main structure of the data. This
helps us create new samples that look like our training data.</p>
<h3 id="key-concepts-of-latent-variables">Key Concepts of Latent
Variables</h3>
<ul>
<li><strong>Latent Space</strong>: This is a simpler version of the data
where the generative process happens. We often learn this space using
methods like variational inference or autoencoders.</li>
<li><strong>Generative Process</strong>: This is how we move from latent
variables to observable data. If ( z ) is our latent variables and ( x )
is our observed data, we can write the generative model as ( p(x | z)
).</li>
</ul>
<h3 id="examples-of-models-utilizing-latent-variables">Examples of
Models Utilizing Latent Variables</h3>
<ol type="1">
<li><strong>Variational Autoencoders (VAEs)</strong>:
<ul>
<li>We encode latent variables from input data. This allows us to make
new data points using a decoder.</li>
<li>The loss function has a part for reconstruction and another part for
regularization (Kullback-Leibler divergence).</li>
</ul>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VAE(nn.Module):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, input_dim, latent_dim):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(VAE, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            nn.Linear(input_dim, <span class="dv">128</span>),</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, latent_dim <span class="op">*</span> <span class="dv">2</span>)  <span class="co"># mean and log variance</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> nn.Sequential(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_dim, <span class="dv">128</span>),</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, input_dim),</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>        params <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>        mu, log_var <span class="op">=</span> params.chunk(<span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu, log_var</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reparameterize(<span class="va">self</span>, mu, log_var):</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> log_var)</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> torch.randn_like(std)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu <span class="op">+</span> eps <span class="op">*</span> std</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.decoder(z)</span></code></pre></div></li>
<li><strong>Generative Adversarial Networks (GANs)</strong>:
<ul>
<li>Here, latent variables are the input for the generator network. It
produces fake data.</li>
<li>The generator tries to create data that looks like the real data
using random noise from the latent space.</li>
</ul>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, latent_dim, output_dim):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Generator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>            nn.Linear(latent_dim, <span class="dv">128</span>),</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, output_dim),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            nn.Tanh()</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z):</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(z)</span></code></pre></div></li>
</ol>
<h3 id="mathematical-representation">Mathematical Representation</h3>
<p>We can show the relationship between latent variables and observed
data like this:</p>
<p>[ p(x) = p(x | z) p(z) dz ]</p>
<p>Here, ( p(z) ) is the prior distribution of the latent variables.
This integral considers all possible configurations of latent variables
that could give us the observed data.</p>
<h3 id="importance-of-latent-variables">Importance of Latent
Variables</h3>
<ul>
<li><strong>Data Compression</strong>: They help reduce dimensions. They
keep important features and remove noise.</li>
<li><strong>Sample Generation</strong>: By sampling from the latent
space, we can create many different and realistic data points.</li>
<li><strong>Interpretability</strong>: Latent variables help us
understand the structure of the dataset. This makes it easier to
understand the data manifold.</li>
</ul>
<p>Latent variables are key parts of modern generative models. They help
us create and represent data in smart ways. If you want to learn more
about the role of latent variables in generative models, you can check
this <a
href="https://bestonlinetutorial.com/generative_ai/what-is-a-variational-autoencoder-vae-and-how-does-it-work-a-comprehensive-guide-to-understanding-vaes.html">comprehensive
guide on variational autoencoders</a>.</p>
<h2
id="exploring-generative-adversarial-networks-and-their-mathematics">Exploring
Generative Adversarial Networks and Their Mathematics</h2>
<p>Generative Adversarial Networks (GANs) are a type of model that
generates new data. They use two neural networks. One is the generator
and the other is the discriminator. These two networks compete with each
other. This competition helps GANs create high-quality fake data that
looks like real data.</p>
<h3 id="mathematical-framework-of-gans">Mathematical Framework of
GANs</h3>
<ul>
<li><p><strong>Objective Function</strong>: The GAN setup is like a
game. The generator ( G ) and the discriminator ( D ) play against each
other. The goal can be written as:</p>
<p>[ <em>G <em>D V(D, G) = </em>{x p</em>{data}(x)}[D(x)] + _{z
p_z(z)}[(1 - D(G(z)))] ]</p>
<p>Here:</p>
<ul>
<li>( p_{data}(x) ) is the real data chance.</li>
<li>( p_z(z) ) is the chance of the hidden space (often Gaussian).</li>
<li>( D(x) ) gives the chance that ( x ) is real.</li>
</ul></li>
</ul>
<h3 id="training-process">Training Process</h3>
<ol type="1">
<li><p><strong>Discriminator Training</strong>:</p>
<ul>
<li>We update ( D ) to make ( V(D, G) ) bigger by telling apart real and
fake data.</li>
<li>The loss function is:</li>
</ul>
<p>[ L_D = -<em>{x p</em>{data}(x)}[D(x)] - _{z p_z(z)}[(1 - D(G(z)))]
]</p></li>
<li><p><strong>Generator Training</strong>:</p>
<ul>
<li>We update ( G ) to make the loss smaller. This makes the
discriminator confused.</li>
<li>The loss function is:</li>
</ul>
<p>[ L_G = -_{z p_z(z)}[D(G(z))] ]</p></li>
</ol>
<h3 id="implementation-in-python-using-tensorflow">Implementation in
Python using TensorFlow</h3>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the generator model</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_generator():</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tf.keras.Sequential()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_dim<span class="op">=</span><span class="dv">100</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">28</span> <span class="op">*</span> <span class="dv">28</span> <span class="op">*</span> <span class="dv">1</span>, activation<span class="op">=</span><span class="st">&#39;tanh&#39;</span>))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Reshape((<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the discriminator model</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_discriminator():</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tf.keras.Sequential()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Flatten(input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>, <span class="dv">1</span>)))</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">512</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">256</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>))</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile models</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> build_generator()</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> build_discriminator()</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>discriminator.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># GAN Model</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>discriminator.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>gan_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="dv">100</span>,))</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>generated_image <span class="op">=</span> generator(gan_input)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>gan_output <span class="op">=</span> discriminator(generated_image)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>gan <span class="op">=</span> tf.keras.Model(gan_input, gan_output)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>gan.<span class="bu">compile</span>(loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>, optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>)</span></code></pre></div>
<h3 id="key-properties">Key Properties</h3>
<ul>
<li><strong>Two-Player Game</strong>: This game setup gives a good way
to make different outputs.</li>
<li><strong>Convergence Issues</strong>: Training can have problems like
mode collapse. This means the generator makes few variations.</li>
<li><strong>Evaluation Metrics</strong>: We use metrics like Inception
Score (IS) and Fr√©chet Inception Distance (FID) to check how good the
generated samples are.</li>
</ul>
<p>The math behind GANs shows how optimization and game theory work
together. This is very important for making good generative models. For
more details about GANs and how to use them, you can see <a
href="https://bestonlinetutorial.com/generative_ai/how-can-gans-be-used-for-super-resolution.html">how
GANs can be used for super-resolution</a>.</p>
<h2 id="an-overview-of-variational-inference-in-generative-models">An
Overview of Variational Inference in Generative Models</h2>
<p>Variational Inference (VI) is a strong technique we use in generative
models. It helps us to estimate complex posterior distributions. VI
changes the inference problem into an optimization problem. We do this
by introducing a family of distributions and finding the best way to
approximate the true posterior.</p>
<h3 id="core-concepts">Core Concepts</h3>
<ul>
<li><p><strong>Latent Variables</strong>: VI works with models that have
latent variables ( z ). These variables are not directly seen. Our aim
is to find the posterior distribution ( p(z|x) ) given the data ( x
).</p></li>
<li><p><strong>Variational Distribution</strong>: We pick a simpler
distribution ( q(z; ) ) to approximate the true posterior. Then we
optimize the parameters ( ).</p></li>
<li><p><strong>Evidence Lower Bound (ELBO)</strong>: Our goal is to
maximize the ELBO. The ELBO is defined as:</p>
<p>[ = <em>{q(z; )}[p(x|z)] - D</em>{KL}(q(z; ) || p(z)) ]</p>
<p>Here, ( D_{KL} ) is the Kullback-Leibler divergence between the
variational distribution and the prior.</p></li>
</ul>
<h3 id="steps-in-variational-inference">Steps in Variational
Inference</h3>
<ol type="1">
<li><p><strong>Define the Model</strong>: We need to specify the
generative model and its latent variables.</p></li>
<li><p><strong>Choose a Variational Family</strong>: We select a form
for ( q(z; ) ). It is often Gaussian because it is easier to work
with.</p></li>
<li><p><strong>Optimize the ELBO</strong>: We can use gradient ascent or
other optimization methods to find the parameters ( ) that increase the
ELBO.</p></li>
</ol>
<h3 id="example-code-in-python">Example Code in Python</h3>
<p>Here we use a simple Gaussian approximation for ( q(z; ) ):</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.optimize <span class="im">as</span> opt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Example generative model parameters</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>mu_prior <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>sigma_prior <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Variational parameters</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>mu_q <span class="op">=</span> np.random.randn()</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>sigma_q <span class="op">=</span> np.random.rand()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> elbo(mu_q, sigma_q):</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute ELBO</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># p(x|z) likelihood term (assumed Gaussian for simplicity)</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    log_likelihood <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> np.<span class="bu">sum</span>((data <span class="op">-</span> mu_q)<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> sigma_q<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># KL divergence term</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    kl_divergence <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (np.log(sigma_prior<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> sigma_q<span class="op">**</span><span class="dv">2</span>) <span class="op">-</span> <span class="dv">1</span> <span class="op">+</span> (sigma_q<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (mu_q <span class="op">-</span> mu_prior)<span class="op">**</span><span class="dv">2</span>) <span class="op">/</span> sigma_prior<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>(log_likelihood <span class="op">-</span> kl_divergence)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimize ELBO</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> opt.minimize(<span class="kw">lambda</span> x: elbo(x[<span class="dv">0</span>], x[<span class="dv">1</span>]), [mu_q, sigma_q], bounds<span class="op">=</span>[(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>), (<span class="fl">1e-5</span>, <span class="dv">10</span>)])</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>optimized_params <span class="op">=</span> result.x</span></code></pre></div>
<h3 id="applications">Applications</h3>
<p>We use Variational Inference in many generative models like:</p>
<ul>
<li><p><strong>Variational Autoencoders (VAEs)</strong>: This framework
combines neural networks with variational inference. It helps us to
learn complex distributions.</p></li>
<li><p><strong>Bayesian Neural Networks</strong>: VI helps to add
uncertainty into the weights of neural networks.</p></li>
</ul>
<p>Using VI in generative models helps us to make efficient inferences
and learn better. It is very important in modern machine learning.</p>
<p>For more information about how to implement Variational Inference,
you can read about <a
href="https://bestonlinetutorial.com/generative_ai/what-is-a-variational-autoencoder-vae-and-how-does-it-work-a-comprehensive-guide-to-understanding-vaes.html">what
is a Variational Autoencoder (VAE) and how does it work?</a>.</p>
<h2
id="mathematical-principles-behind-autoencoders-in-generative-models">Mathematical
Principles Behind Autoencoders in Generative Models</h2>
<p>Autoencoders are a type of artificial neural networks. We use them to
learn good representations of data. This is often for reducing
dimensions or learning features. Autoencoders have two main parts: the
encoder and the decoder. We can understand the math behind autoencoders
through important ideas like reconstruction loss, latent space
representation, and activation functions.</p>
<h3 id="encoder-and-decoder">Encoder and Decoder</h3>
<ol type="1">
<li><p><strong>Encoder</strong>: This part maps the input ( x ) to a
latent representation ( z ): [ z = f_{}(x) = (W_{}x + b_{}) ] Here, (
W_{} ) is the weight matrix. ( b_{} ) is the bias. ( ) is usually a
non-linear activation function. Examples are ReLU or sigmoid.</p></li>
<li><p><strong>Decoder</strong>: This part maps the latent
representation ( z ) back to the original space: [ = f_{}(z) = (W_{}z +
b_{}) ] In this case, ( W_{} ) and ( b_{} ) are the weights and biases
of the decoder.</p></li>
</ol>
<h3 id="reconstruction-loss">Reconstruction Loss</h3>
<p>The goal of an autoencoder is to make the input ( x ) and the output
( ) as close as possible. We usually do this using a loss function, like
Mean Squared Error (MSE): [ L(x, ) = ||x - ||^2 = _{i=1}^{n}(x_i - _i)^2
] Here, ( n ) is the number of features in the input data.</p>
<h3 id="latent-space-representation">Latent Space Representation</h3>
<p>The latent space ( z ) is a smaller version of the input data. It
keeps the most important features. The size of ( z ) is less than ( x ).
This helps us represent data better.</p>
<h3 id="activation-functions">Activation Functions</h3>
<p>Some common activation functions we use in autoencoders are: -
<strong>ReLU</strong>: ( f(x) = (0, x) ) - <strong>Sigmoid</strong>: (
f(x) = ) - <strong>Tanh</strong>: ( f(x) = )</p>
<h3 id="regularization-techniques">Regularization Techniques</h3>
<p>To stop overfitting, we can use different regularization methods
like: - <strong>L2 Regularization</strong>: This adds a penalty on the
size of weights. - <strong>Dropout</strong>: Here, we randomly set some
input units to 0 during training. - <strong>Denoising
Autoencoders</strong>: This adds noise to the input data. Then, we train
the autoencoder to recreate the original input.</p>
<h3 id="implementation-example">Implementation Example</h3>
<p>Here is a simple example of an autoencoder using
TensorFlow/Keras:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Input, Dense</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Model</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input and encoding dimensions</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>input_dim <span class="op">=</span> <span class="dv">784</span>  <span class="co"># Example for MNIST dataset</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>encoding_dim <span class="op">=</span> <span class="dv">32</span>  <span class="co"># Size of the latent space</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the input layer</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>input_layer <span class="op">=</span> Input(shape<span class="op">=</span>(input_dim,))</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Encoder</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>encoded <span class="op">=</span> Dense(encoding_dim, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(input_layer)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Decoder</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>decoded <span class="op">=</span> Dense(input_dim, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)(encoded)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Autoencoder model</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="op">=</span> Model(input_layer, decoded)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the model</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>autoencoder.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, loss<span class="op">=</span><span class="st">&#39;mean_squared_error&#39;</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of fitting the model</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># x_train should be your training data</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co"># autoencoder.fit(x_train, x_train, epochs=50, batch_size=256, shuffle=True)</span></span></code></pre></div>
<p>This code shows the basic structure of an autoencoder. The model
learns to reduce the reconstruction loss. By knowing these math ideas,
we can use autoencoders in many generative modeling tasks. For more
about generative models and how to use them, check <a
href="https://bestonlinetutorial.com/generative_ai/what-is-a-variational-autoencoder-vae-and-how-does-it-work-a-comprehensive-guide-to-understanding-vaes.html">this
guide on Variational Autoencoders</a>.</p>
<h2 id="practical-examples-of-generative-models-in-python">Practical
Examples of Generative Models in Python</h2>
<p>Generative models are very important in machine learning. They help
us create new data samples. Here, we show some easy examples of how to
use generative models in Python. We will use popular libraries like
TensorFlow and PyTorch.</p>
<h3 id="generative-adversarial-networks-gans">1. Generative Adversarial
Networks (GANs)</h3>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_generator():</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tf.keras.Sequential()</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, input_shape<span class="op">=</span>(<span class="dv">100</span>,)))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">784</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>))</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Reshape((<span class="dv">28</span>, <span class="dv">28</span>)))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_discriminator():</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> tf.keras.Sequential()</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Flatten(input_shape<span class="op">=</span>(<span class="dv">28</span>, <span class="dv">28</span>)))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">128</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>))</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    model.add(layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> build_generator()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> build_discriminator()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the discriminator</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>discriminator.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create GAN model</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>discriminator.trainable <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>gan_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(<span class="dv">100</span>,))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>generated_image <span class="op">=</span> generator(gan_input)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>gan_output <span class="op">=</span> discriminator(generated_image)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>gan <span class="op">=</span> tf.keras.Model(gan_input, gan_output)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>gan.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;adam&#39;</span>, loss<span class="op">=</span><span class="st">&#39;binary_crossentropy&#39;</span>)</span></code></pre></div>
<h3 id="variational-autoencoders-vaes">2. Variational Autoencoders
(VAEs)</h3>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers, Model</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> build_vae(input_shape, latent_dim):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> layers.Input(shape<span class="op">=</span>input_shape)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> layers.Flatten()(inputs)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    z_mean <span class="op">=</span> layers.Dense(latent_dim)(x)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    z_log_var <span class="op">=</span> layers.Dense(latent_dim)(x)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sampling(args):</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        z_mean, z_log_var <span class="op">=</span> args</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> tf.keras.backend.random_normal(shape<span class="op">=</span>tf.shape(z_mean))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> z_mean <span class="op">+</span> tf.exp(<span class="fl">0.5</span> <span class="op">*</span> z_log_var) <span class="op">*</span> epsilon</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> layers.Lambda(sampling)([z_mean, z_log_var])</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    encoder <span class="op">=</span> Model(inputs, [z_mean, z_log_var, z])</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    decoder_input <span class="op">=</span> layers.Input(shape<span class="op">=</span>(latent_dim,))</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>)(decoder_input)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> layers.Dense(np.prod(input_shape), activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> layers.Reshape(input_shape)(x)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    decoder <span class="op">=</span> Model(decoder_input, outputs)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> encoder, decoder</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>vae_encoder, vae_decoder <span class="op">=</span> build_vae((<span class="dv">28</span>, <span class="dv">28</span>), <span class="dv">2</span>)</span></code></pre></div>
<h3 id="using-pytorch-for-gans">3. Using PyTorch for GANs</h3>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Generator(nn.Module):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Generator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">100</span>, <span class="dv">128</span>),</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">784</span>),</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>            nn.Tanh()</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z):</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(z)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Discriminator, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">784</span>, <span class="dv">128</span>),</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">128</span>, <span class="dv">1</span>),</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            nn.Sigmoid()</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> Generator()</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> Discriminator()</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>optimizer_G <span class="op">=</span> optim.Adam(generator.parameters(), lr<span class="op">=</span><span class="fl">0.0002</span>, betas<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>optimizer_D <span class="op">=</span> optim.Adam(discriminator.parameters(), lr<span class="op">=</span><span class="fl">0.0002</span>, betas<span class="op">=</span>(<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span></code></pre></div>
<h3 id="training-a-gan">4. Training a GAN</h3>
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_gan(generator, discriminator, epochs<span class="op">=</span><span class="dv">10000</span>, batch_size<span class="op">=</span><span class="dv">64</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train Discriminator</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn(batch_size, <span class="dv">100</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        fake_images <span class="op">=</span> generator(noise)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        real_labels <span class="op">=</span> torch.ones(batch_size, <span class="dv">1</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        fake_labels <span class="op">=</span> torch.zeros(batch_size, <span class="dv">1</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        optimizer_D.zero_grad()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> discriminator(fake_images.detach())</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        d_loss_fake <span class="op">=</span> nn.BCELoss()(output, fake_labels)</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> discriminator(real_images)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        d_loss_real <span class="op">=</span> nn.BCELoss()(output, real_labels)</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        d_loss <span class="op">=</span> d_loss_fake <span class="op">+</span> d_loss_real</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        d_loss.backward()</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        optimizer_D.step()</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Train Generator</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>        optimizer_G.zero_grad()</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> discriminator(fake_images)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        g_loss <span class="op">=</span> nn.BCELoss()(output, real_labels)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        g_loss.backward()</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        optimizer_G.step()</span></code></pre></div>
<p>These examples show how to use generative models in Python. We learn
how to build and train a GAN and a VAE. For more details about
generative models and how they work, we can check related topics. For
example, we can look at <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-steps-to-implement-a-simple-generative-model-from-scratch.html">the
steps to implement a simple generative model from scratch</a> and <a
href="https://bestonlinetutorial.com/generative_ai/what-is-a-variational-autoencoder-vae-and-how-does-it-work-a-comprehensive-guide-to-understanding-vaes.html">the
guide on Variational Autoencoders</a>.</p>
<h2 id="how-do-generative-models-learn-from-data">How Do Generative
Models Learn from Data?</h2>
<p>Generative models learn from data by estimating probability
distributions that describe the data. We use different math and computer
techniques to help them create new samples that look like the training
data. Here are the main steps involved:</p>
<ol type="1">
<li><p><strong>Data Representation</strong>: The model learns to show
the input data in a way that highlights the important features. We can
use methods like feature extraction or reducing dimensions.</p></li>
<li><p><strong>Parameter Estimation</strong>: Generative models have
parameters that we need to estimate from the data. We can use methods
like Maximum Likelihood Estimation (MLE) or Bayesian inference.</p>
<ul>
<li><strong>Maximum Likelihood Estimation</strong>: [ = _{} P(X | ) ]
Here, ( X ) is the data we see and ( ) are the model parameters.</li>
</ul></li>
<li><p><strong>Learning the Distribution</strong>: The model learns the
joint probability distribution ( P(X, Y) ) of the data ( X ) and hidden
variables ( Y ). We can use techniques like Gaussian Mixture Models
(GMM) or Variational Autoencoders (VAEs).</p></li>
<li><p><strong>Latent Variable Modeling</strong>: In many generative
models, we add latent variables to find hidden patterns in the data. The
model learns the distribution of these variables, often using methods
like Variational Inference.</p></li>
<li><p><strong>Iterative Optimization</strong>: The learning process
often uses iterative optimization methods like Gradient Descent. For
example, in a Generative Adversarial Network (GAN), we train two neural
networks, the generator and the discriminator, in opposition.</p>
<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a simple GAN training loop</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    noise <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, (batch_size, noise_dim))</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    generated_samples <span class="op">=</span> generator.predict(noise)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    real_samples <span class="op">=</span> get_real_samples(batch_size)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    d_loss_real <span class="op">=</span> discriminator.train_on_batch(real_samples, real_labels)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    d_loss_fake <span class="op">=</span> discriminator.train_on_batch(generated_samples, fake_labels)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    g_loss <span class="op">=</span> gan.train_on_batch(noise, real_labels)</span></code></pre></div></li>
<li><p><strong>Evaluation and Tuning</strong>: After training, we check
how well the model generates new data using metrics like Inception Score
(IS) or Fr√©chet Inception Distance (FID). We may also tune
hyperparameters to make the learning better.</p></li>
<li><p><strong>Data Augmentation</strong>: Generative models can learn
by adding variations to the data. This helps improve the model‚Äôs
strength and performance.</p></li>
</ol>
<p>By using these techniques, generative models learn from data well.
They can create new, synthetic samples that look like the original data.
This skill is important for many uses, such as image generation, text
creation, and more. For more insights on generative models, we can check
resources like <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-key-differences-between-generative-and-discriminative-models-understanding-their-unique-features-and-applications.html">what
are the key differences between generative and discriminative
models</a>.</p>
<h2 id="frequently-asked-questions">Frequently Asked Questions</h2>
<h3 id="what-are-generative-models-in-machine-learning">1. What are
generative models in machine learning?</h3>
<p>Generative models in machine learning are simple models that create
new data points from existing data. They learn the main patterns in the
data. This lets them make samples that look like the original dataset.
Some common types are Generative Adversarial Networks (GANs) and
Variational Autoencoders (VAEs). It is important to know the basic math
behind generative models. This helps us use them well in things like
making images and adding more data.</p>
<h3 id="how-do-generative-models-learn-from-data-1">2. How do generative
models learn from data?</h3>
<p>Generative models learn from data by finding out the probability
distribution behind the dataset. They use methods like maximum
likelihood estimation or variational inference to change their settings.
During training, these models keep improving their skill to create data
that matches the input data. This makes them useful for tasks like
making images and generating text. For more details, check out <a
href="https://bestonlinetutorial.com/generative_ai/what-are-the-steps-to-implement-a-simple-generative-model-from-scratch.html">how
to implement a simple generative model from scratch</a>.</p>
<h3 id="what-is-the-role-of-latent-variables-in-generative-models">3.
What is the role of latent variables in generative models?</h3>
<p>Latent variables in generative models are hidden variables that help
show the main structure of the data. They help the model handle complex
data distributions better. For example, in Variational Autoencoders
(VAEs), latent variables let the model learn a smaller version of the
input data. This smaller version can then be used to create new and
similar data points. Knowing about latent variables is key to
understanding the math behind generative models.</p>
<h3 id="how-do-generative-adversarial-networks-gans-work">4. How do
Generative Adversarial Networks (GANs) work?</h3>
<p>Generative Adversarial Networks (GANs) have two neural networks: a
generator and a discriminator. The generator makes new data samples. The
discriminator checks these samples against real data. This
back-and-forth continues until the generator makes samples that look
just like real data. The math behind GANs includes ideas from game
theory and optimization. This makes them a great choice for creating
realistic images and videos. For a step-by-step guide on training a GAN,
visit <a
href="https://bestonlinetutorial.com/generative_ai/how-can-you-train-a-gan-a-step-by-step-tutorial-guide.html">this
guide</a>.</p>
<h3 id="what-is-variational-inference-in-generative-models">5. What is
variational inference in generative models?</h3>
<p>Variational inference is a way to get close to complex posterior
distributions in generative models. It turns the inference problem into
an optimization problem. This is done by adding a simpler distribution,
which makes calculations easier. This method is very useful in models
like Variational Autoencoders (VAEs). It helps us learn the
representations of latent variables more efficiently. For more insights
into VAEs and how they work, check out <a
href="https://bestonlinetutorial.com/generative_ai/what-is-a-variational-autoencoder-vae-and-how-does-it-work-a-comprehensive-guide-to-understanding-vaes.html">this
comprehensive guide</a>.</p>

                        </div>

                    </div>
                    <!--//container-->
                </article>

            </div>
            <!--//main-wrapper-->

            <div id="footer-placeholder"></div>

            <!-- Javascript -->
            <script src="/assets/plugins/popper.min.js" defer></script>
            <script src="/assets/plugins/bootstrap/js/bootstrap.min.js" defer></script>
            <script src="/assets/fontawesome/js/all.min.js" defer></script>
        </body>

        </html>
            
            